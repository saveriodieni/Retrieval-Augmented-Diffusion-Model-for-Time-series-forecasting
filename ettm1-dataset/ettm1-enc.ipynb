{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc1d880",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2f0f9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:00:55.933379Z",
     "iopub.status.busy": "2025-09-13T12:00:55.933198Z",
     "iopub.status.idle": "2025-09-13T12:01:00.278197Z",
     "shell.execute_reply": "2025-09-13T12:01:00.277427Z",
     "shell.execute_reply.started": "2025-09-13T12:00:55.933361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a740f43",
   "metadata": {},
   "source": [
    "# Data extraction and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a717c9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:00.279386Z",
     "iopub.status.busy": "2025-09-13T12:01:00.279021Z",
     "iopub.status.idle": "2025-09-13T12:01:00.548430Z",
     "shell.execute_reply": "2025-09-13T12:01:00.547723Z",
     "shell.execute_reply.started": "2025-09-13T12:01:00.279367Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date   HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
      "0  2016-07-01 00:00:00  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
      "1  2016-07-01 00:15:00  5.760  2.076  1.492  0.426  4.264  1.401  30.459999\n",
      "2  2016-07-01 00:30:00  5.760  1.942  1.492  0.391  4.234  1.310  30.038000\n",
      "3  2016-07-01 00:45:00  5.760  1.942  1.492  0.426  4.234  1.310  27.013000\n",
      "4  2016-07-01 01:00:00  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV\n",
    "csv_file = \"/kaggle/input/eetm-1/ETTm1.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5d1845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:00.550463Z",
     "iopub.status.busy": "2025-09-13T12:01:00.550050Z",
     "iopub.status.idle": "2025-09-13T12:01:00.553855Z",
     "shell.execute_reply": "2025-09-13T12:01:00.553035Z",
     "shell.execute_reply.started": "2025-09-13T12:01:00.550443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "window_length = 168\n",
    "step_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3db128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:00.555122Z",
     "iopub.status.busy": "2025-09-13T12:01:00.554768Z",
     "iopub.status.idle": "2025-09-13T12:01:03.610007Z",
     "shell.execute_reply": "2025-09-13T12:01:03.609306Z",
     "shell.execute_reply.started": "2025-09-13T12:01:00.555097Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (69513, 168, 7)\n",
      "Shape of meta_time: (69513, 168)\n"
     ]
    }
   ],
   "source": [
    "# 1. Order by date\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "def extract_windows(df, window_len, step):\n",
    "    windows = []\n",
    "    timestamps = []\n",
    "    \n",
    "    # Drop non-numeric columns for window extraction\n",
    "    numeric_df = df.drop(columns=[\"date\"])\n",
    "    \n",
    "    for i in range(step - 1, len(df) - window_len + 1, step):\n",
    "        # Window of numeric features\n",
    "        window = numeric_df.iloc[i:i+window_len].values\n",
    "        windows.append(window)\n",
    "        \n",
    "        # Associate timestamps\n",
    "        timestamps.append(df[\"date\"].iloc[i:i+window_len].values)\n",
    "    \n",
    "    return np.array(windows), np.array(timestamps)\n",
    "\n",
    "all_windows, all_timestamps = extract_windows(df, window_length, step_size)\n",
    "\n",
    "X = all_windows  # shape: (num_samples, window_length, num_features)\n",
    "meta_time = all_timestamps  # metadata: array of associated dates\n",
    "\n",
    "print(\"Shape of X:\", X.shape)               # (num_samples, window_length, num_features)\n",
    "print(\"Shape of meta_time:\", meta_time.shape)  # (num_samples, window_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3e4b15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:03.611154Z",
     "iopub.status.busy": "2025-09-13T12:01:03.610913Z",
     "iopub.status.idle": "2025-09-13T12:01:04.325538Z",
     "shell.execute_reply": "2025-09-13T12:01:04.324739Z",
     "shell.execute_reply.started": "2025-09-13T12:01:03.611131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, meta_time = shuffle(X, meta_time, random_state=626)\n",
    "\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.1 * len(X))\n",
    "test_size = len(X) - train_size - val_size\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:train_size + val_size]\n",
    "X_test = X[train_size + val_size:]\n",
    "\n",
    "meta_time_train = meta_time[:train_size]\n",
    "meta_time_val = meta_time[train_size:train_size + val_size]\n",
    "meta_time_test = meta_time[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661cfd8b",
   "metadata": {},
   "source": [
    "# Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359a70b",
   "metadata": {},
   "source": [
    "Our goal was to build an encoder-decoder model able to learn a compressed representaion of the input time series, so to allow a more efficient search of similar time series in a smaller dimensional space, speeding up the task of finding k nearest neighbours. </br> The encoder gets as input a tensor of shape (batch_size, seq_len, num_features) and compresses it into a tensor of shape (batch_size, embedding_dim), while the decoder takes the output of the encoder and tries to reconstruct the original input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a9a7560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:04.326829Z",
     "iopub.status.busy": "2025-09-13T12:01:04.326388Z",
     "iopub.status.idle": "2025-09-13T12:01:04.341554Z",
     "shell.execute_reply": "2025-09-13T12:01:04.340857Z",
     "shell.execute_reply.started": "2025-09-13T12:01:04.326801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size] if self.chomp_size > 0 else x\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) * dilation  # full causal\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                               padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "            self.conv2, self.chomp2, self.relu2, self.dropout2,\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        out = self.net(x)\n",
    "        if out.shape != res.shape:\n",
    "            # Align time dimension by cropping the residual (this might be necessary in some edge cases)\n",
    "            min_len = min(out.size(-1), res.size(-1))\n",
    "            out = out[..., :min_len]\n",
    "            res = res[..., :min_len]\n",
    "        return self.relu(out + res)\n",
    "\n",
    "# Encoder\n",
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(num_channels)):\n",
    "            in_ch = input_dim if i == 0 else num_channels[i - 1]\n",
    "            out_ch = num_channels[i]\n",
    "            dilation = 2 ** i\n",
    "            layers.append(TemporalBlock(in_ch, out_ch, kernel_size, dilation, dropout))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "\n",
    "        # Projection from [B, C, T] to [B, emb_dim]\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.project = nn.Linear(num_channels[-1], emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, D] â†’ [B, D, T]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.tcn(x)  # [B, C, T]\n",
    "        x = self.pool(x).squeeze(-1)  # [B, C]\n",
    "        x = self.project(x)  # [B, emb_dim]\n",
    "        return x\n",
    "\n",
    "# Decoder\n",
    "class TCNDecoder(nn.Module):\n",
    "    def __init__(self, emb_dim, output_dim, seq_len, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Project embedding back to a sequence shape: [B, C, T]\n",
    "        self.expand = nn.Linear(emb_dim, num_channels[0] * seq_len)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(num_channels) - 1):\n",
    "            in_ch = num_channels[i]\n",
    "            out_ch = num_channels[i + 1]\n",
    "            dilation = 2 ** i\n",
    "            layers.append(TemporalBlock(in_ch, out_ch, kernel_size, dilation, dropout))\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.out_proj = nn.Conv1d(num_channels[-1], output_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, emb_dim] â†’ [B, C0, T]\n",
    "        x = self.expand(x)  # [B, C0 * T]\n",
    "        x = x.view(x.size(0), -1, self.seq_len)  # [B, C0, T]\n",
    "        x = self.tcn(x)  # [B, Cn, T]\n",
    "        x = self.out_proj(x)  # [B, D, T]\n",
    "        return x.permute(0, 2, 1)  # [B, T, D]\n",
    "    \n",
    "class TCNAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, seq_len, channels, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = TCNEncoder(input_dim, emb_dim, channels, kernel_size, dropout)\n",
    "        self.decoder = TCNDecoder(emb_dim, input_dim, seq_len, channels[::-1], kernel_size, dropout)\n",
    "\n",
    "    def forward(self, x, only_encoder = False):\n",
    "        # x: [B, T, D]\n",
    "        z = self.encoder(x)       # [B, emb_dim]\n",
    "        if only_encoder:\n",
    "            return z\n",
    "        x_recon = self.decoder(z) # [B, T, D]\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d42669af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:04.342584Z",
     "iopub.status.busy": "2025-09-13T12:01:04.342312Z",
     "iopub.status.idle": "2025-09-13T12:01:05.458397Z",
     "shell.execute_reply": "2025-09-13T12:01:05.457644Z",
     "shell.execute_reply.started": "2025-09-13T12:01:04.342563Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 7.41981425  2.25796281  4.32981479  0.89406359  3.07951596\n",
      "    0.856835   13.34120937]]] [[[7.0826444  2.04137929 6.82801614 1.80879328 1.17256459 0.59970438\n",
      "   8.58232446]]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the statistics of the train dataset and normalize with respect to it \n",
    "\n",
    "mean = X_train.mean(axis=(0, 1), keepdims=True)  # shape (1, 1, num_features)\n",
    "std = X_train.std(axis=(0, 1), keepdims=True)\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_val = (X_val - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bdd7eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:05.459538Z",
     "iopub.status.busy": "2025-09-13T12:01:05.459268Z",
     "iopub.status.idle": "2025-09-13T12:01:05.643595Z",
     "shell.execute_reply": "2025-09-13T12:01:05.642989Z",
     "shell.execute_reply.started": "2025-09-13T12:01:05.459507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Dataset & DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec739f61",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac80033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:05.646107Z",
     "iopub.status.busy": "2025-09-13T12:01:05.645887Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,569,895\n",
      "Models saved successfully.\n",
      "Epoch 01 | Train MSE: 0.338877 | Val MSE: 0.205334 | Test MSE: 0.206716\n",
      "Models saved successfully.\n",
      "Epoch 02 | Train MSE: 0.179992 | Val MSE: 0.151248 | Test MSE: 0.153404\n",
      "Models saved successfully.\n",
      "Epoch 03 | Train MSE: 0.147473 | Val MSE: 0.130057 | Test MSE: 0.131775\n",
      "Models saved successfully.\n",
      "Epoch 04 | Train MSE: 0.127477 | Val MSE: 0.114420 | Test MSE: 0.115974\n",
      "Models saved successfully.\n",
      "Epoch 05 | Train MSE: 0.114159 | Val MSE: 0.106209 | Test MSE: 0.108207\n",
      "Models saved successfully.\n",
      "Epoch 06 | Train MSE: 0.105806 | Val MSE: 0.099056 | Test MSE: 0.100021\n",
      "Models saved successfully.\n",
      "Epoch 07 | Train MSE: 0.098935 | Val MSE: 0.090676 | Test MSE: 0.091579\n",
      "Models saved successfully.\n",
      "Epoch 08 | Train MSE: 0.093704 | Val MSE: 0.087473 | Test MSE: 0.088380\n",
      "Models saved successfully.\n",
      "Epoch 09 | Train MSE: 0.089430 | Val MSE: 0.080889 | Test MSE: 0.081535\n",
      "Models saved successfully.\n",
      "Epoch 10 | Train MSE: 0.085954 | Val MSE: 0.080240 | Test MSE: 0.080947\n",
      "Models saved successfully.\n",
      "Epoch 11 | Train MSE: 0.083043 | Val MSE: 0.078457 | Test MSE: 0.078781\n",
      "Epoch 12 | Train MSE: 0.080803 | Val MSE: 0.080573 | Test MSE: 0.080870\n",
      "Models saved successfully.\n",
      "Epoch 13 | Train MSE: 0.078298 | Val MSE: 0.077721 | Test MSE: 0.078158\n",
      "Models saved successfully.\n",
      "Epoch 14 | Train MSE: 0.077060 | Val MSE: 0.069980 | Test MSE: 0.070526\n",
      "Models saved successfully.\n",
      "Epoch 15 | Train MSE: 0.075320 | Val MSE: 0.069642 | Test MSE: 0.070017\n",
      "Models saved successfully.\n",
      "Epoch 16 | Train MSE: 0.073816 | Val MSE: 0.069291 | Test MSE: 0.069708\n",
      "Models saved successfully.\n",
      "Epoch 17 | Train MSE: 0.072396 | Val MSE: 0.067949 | Test MSE: 0.068358\n",
      "Models saved successfully.\n",
      "Epoch 18 | Train MSE: 0.071185 | Val MSE: 0.065863 | Test MSE: 0.066302\n",
      "Epoch 19 | Train MSE: 0.070350 | Val MSE: 0.068826 | Test MSE: 0.069206\n",
      "Epoch 20 | Train MSE: 0.069103 | Val MSE: 0.066659 | Test MSE: 0.066783\n",
      "Models saved successfully.\n",
      "Epoch 21 | Train MSE: 0.068320 | Val MSE: 0.063931 | Test MSE: 0.064152\n",
      "Models saved successfully.\n",
      "Epoch 22 | Train MSE: 0.067165 | Val MSE: 0.062399 | Test MSE: 0.062747\n",
      "Models saved successfully.\n",
      "Epoch 23 | Train MSE: 0.066460 | Val MSE: 0.061815 | Test MSE: 0.062103\n",
      "Epoch 24 | Train MSE: 0.065711 | Val MSE: 0.062313 | Test MSE: 0.062683\n",
      "Epoch 25 | Train MSE: 0.065262 | Val MSE: 0.062425 | Test MSE: 0.062705\n",
      "Models saved successfully.\n",
      "Epoch 26 | Train MSE: 0.064293 | Val MSE: 0.060611 | Test MSE: 0.060904\n",
      "Models saved successfully.\n",
      "Epoch 27 | Train MSE: 0.063696 | Val MSE: 0.060231 | Test MSE: 0.060489\n",
      "Models saved successfully.\n",
      "Epoch 28 | Train MSE: 0.063187 | Val MSE: 0.059436 | Test MSE: 0.059818\n",
      "Epoch 29 | Train MSE: 0.062673 | Val MSE: 0.060027 | Test MSE: 0.060341\n",
      "Models saved successfully.\n",
      "Epoch 30 | Train MSE: 0.062208 | Val MSE: 0.058222 | Test MSE: 0.058624\n",
      "Epoch 31 | Train MSE: 0.061676 | Val MSE: 0.059153 | Test MSE: 0.059428\n",
      "Epoch 32 | Train MSE: 0.061126 | Val MSE: 0.058698 | Test MSE: 0.058984\n",
      "Epoch 33 | Train MSE: 0.060768 | Val MSE: 0.058216 | Test MSE: 0.058754\n",
      "Models saved successfully.\n",
      "Epoch 34 | Train MSE: 0.060291 | Val MSE: 0.056564 | Test MSE: 0.056874\n",
      "Epoch 35 | Train MSE: 0.059778 | Val MSE: 0.057099 | Test MSE: 0.057417\n",
      "Models saved successfully.\n",
      "Epoch 36 | Train MSE: 0.059446 | Val MSE: 0.055698 | Test MSE: 0.056127\n",
      "Models saved successfully.\n",
      "Epoch 37 | Train MSE: 0.059279 | Val MSE: 0.055209 | Test MSE: 0.055594\n",
      "Models saved successfully.\n",
      "Epoch 38 | Train MSE: 0.058729 | Val MSE: 0.054650 | Test MSE: 0.055134\n",
      "Epoch 39 | Train MSE: 0.058571 | Val MSE: 0.055747 | Test MSE: 0.056053\n",
      "Epoch 40 | Train MSE: 0.058262 | Val MSE: 0.056220 | Test MSE: 0.056579\n",
      "Models saved successfully.\n",
      "Epoch 41 | Train MSE: 0.057854 | Val MSE: 0.054327 | Test MSE: 0.054619\n",
      "Models saved successfully.\n",
      "Epoch 42 | Train MSE: 0.057624 | Val MSE: 0.054003 | Test MSE: 0.054341\n",
      "Epoch 43 | Train MSE: 0.057362 | Val MSE: 0.056001 | Test MSE: 0.056404\n",
      "Epoch 44 | Train MSE: 0.057106 | Val MSE: 0.054006 | Test MSE: 0.054416\n",
      "Epoch 45 | Train MSE: 0.056850 | Val MSE: 0.054281 | Test MSE: 0.054525\n",
      "Epoch 46 | Train MSE: 0.056523 | Val MSE: 0.054569 | Test MSE: 0.054992\n",
      "Epoch 47 | Train MSE: 0.056497 | Val MSE: 0.054012 | Test MSE: 0.054383\n",
      "Epoch 48 | Train MSE: 0.056169 | Val MSE: 0.053979 | Test MSE: 0.054411\n",
      "Epoch 49 | Train MSE: 0.055938 | Val MSE: 0.054227 | Test MSE: 0.054807\n",
      "Models saved successfully.\n",
      "Epoch 50 | Train MSE: 0.055665 | Val MSE: 0.052400 | Test MSE: 0.052762\n",
      "Epoch 51 | Train MSE: 0.055405 | Val MSE: 0.052817 | Test MSE: 0.053195\n",
      "Epoch 52 | Train MSE: 0.055195 | Val MSE: 0.053390 | Test MSE: 0.053741\n",
      "Epoch 53 | Train MSE: 0.055064 | Val MSE: 0.052729 | Test MSE: 0.053226\n",
      "Models saved successfully.\n",
      "Epoch 54 | Train MSE: 0.054749 | Val MSE: 0.051987 | Test MSE: 0.052304\n",
      "Epoch 55 | Train MSE: 0.054846 | Val MSE: 0.053115 | Test MSE: 0.053680\n",
      "Epoch 56 | Train MSE: 0.054544 | Val MSE: 0.053427 | Test MSE: 0.053707\n",
      "Epoch 57 | Train MSE: 0.054262 | Val MSE: 0.052163 | Test MSE: 0.052486\n",
      "Models saved successfully.\n",
      "Epoch 58 | Train MSE: 0.054325 | Val MSE: 0.051806 | Test MSE: 0.052160\n",
      "Epoch 59 | Train MSE: 0.054019 | Val MSE: 0.055341 | Test MSE: 0.055828\n",
      "Epoch 60 | Train MSE: 0.054056 | Val MSE: 0.051854 | Test MSE: 0.052295\n",
      "Models saved successfully.\n",
      "Epoch 61 | Train MSE: 0.053724 | Val MSE: 0.051109 | Test MSE: 0.051587\n",
      "Epoch 62 | Train MSE: 0.053529 | Val MSE: 0.051974 | Test MSE: 0.052251\n",
      "Epoch 63 | Train MSE: 0.053685 | Val MSE: 0.052812 | Test MSE: 0.053165\n",
      "Models saved successfully.\n",
      "Epoch 64 | Train MSE: 0.053108 | Val MSE: 0.050823 | Test MSE: 0.051181\n",
      "Epoch 65 | Train MSE: 0.053170 | Val MSE: 0.050949 | Test MSE: 0.051314\n",
      "Epoch 66 | Train MSE: 0.053004 | Val MSE: 0.051048 | Test MSE: 0.051462\n",
      "Models saved successfully.\n",
      "Epoch 67 | Train MSE: 0.052884 | Val MSE: 0.050539 | Test MSE: 0.050817\n",
      "Epoch 68 | Train MSE: 0.052713 | Val MSE: 0.051249 | Test MSE: 0.051591\n",
      "Epoch 69 | Train MSE: 0.052687 | Val MSE: 0.051534 | Test MSE: 0.051804\n",
      "Models saved successfully.\n",
      "Epoch 70 | Train MSE: 0.052380 | Val MSE: 0.050461 | Test MSE: 0.050724\n",
      "Epoch 71 | Train MSE: 0.052523 | Val MSE: 0.051866 | Test MSE: 0.052326\n",
      "Epoch 72 | Train MSE: 0.052267 | Val MSE: 0.051086 | Test MSE: 0.051368\n"
     ]
    }
   ],
   "source": [
    "seq_len = X_train.shape[1]\n",
    "num_features = X_train.shape[2]\n",
    "\n",
    "# Parameters\n",
    "B, T, D = batch_size, seq_len, num_features\n",
    "emb_dim = 64\n",
    "channels = [32, 64, 128]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate model\n",
    "model = TCNAutoencoder(input_dim=D, emb_dim=emb_dim, seq_len=T, channels=channels).to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "params = list(model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "best_loss = 1e10\n",
    "\n",
    "encoder_path = \"/kaggle/working/encoder.pth\"\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (x,) in loader:\n",
    "            x = x.to(device)\n",
    "            x_hat = model(x)\n",
    "            loss = loss_fn(x_hat, x)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for (x,) in train_loader:\n",
    "        x = x.to(device)\n",
    "        x_hat = model(x)\n",
    "        loss = loss_fn(x_hat, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    test_loss = evaluate(model, test_loader)\n",
    "\n",
    "    if val_loss + test_loss < best_loss:\n",
    "        best_loss = val_loss + test_loss\n",
    "        # Saves the weights\n",
    "        torch.save(model.state_dict(), encoder_path)\n",
    "        print(\"Models saved successfully.\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f} | Test MSE: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ab7fee-323d-45da-bb15-a76f734a8faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:21:54.650209Z",
     "iopub.status.busy": "2025-09-13T12:21:54.649610Z",
     "iopub.status.idle": "2025-09-13T12:28:19.577209Z",
     "shell.execute_reply": "2025-09-13T12:28:19.576351Z",
     "shell.execute_reply.started": "2025-09-13T12:21:54.650186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,569,895\n",
      "Models saved successfully.\n",
      "Epoch 71 | Train MSE: 0.051743 | Val MSE: 0.049214 | Test MSE: 0.049501\n",
      "Epoch 72 | Train MSE: 0.051660 | Val MSE: 0.049366 | Test MSE: 0.049669\n",
      "Epoch 73 | Train MSE: 0.051607 | Val MSE: 0.050077 | Test MSE: 0.050378\n",
      "Epoch 74 | Train MSE: 0.051510 | Val MSE: 0.051597 | Test MSE: 0.052141\n",
      "Epoch 75 | Train MSE: 0.051365 | Val MSE: 0.049275 | Test MSE: 0.049601\n",
      "Epoch 76 | Train MSE: 0.051322 | Val MSE: 0.049512 | Test MSE: 0.049924\n",
      "Epoch 77 | Train MSE: 0.051076 | Val MSE: 0.049709 | Test MSE: 0.049980\n",
      "Epoch 78 | Train MSE: 0.051082 | Val MSE: 0.049299 | Test MSE: 0.049590\n",
      "Models saved successfully.\n",
      "Epoch 79 | Train MSE: 0.051025 | Val MSE: 0.049136 | Test MSE: 0.049501\n",
      "Epoch 80 | Train MSE: 0.050776 | Val MSE: 0.049907 | Test MSE: 0.050291\n",
      "Epoch 81 | Train MSE: 0.050809 | Val MSE: 0.049684 | Test MSE: 0.050038\n",
      "Models saved successfully.\n",
      "Epoch 82 | Train MSE: 0.050539 | Val MSE: 0.048836 | Test MSE: 0.049095\n",
      "Epoch 83 | Train MSE: 0.050689 | Val MSE: 0.049627 | Test MSE: 0.049832\n",
      "Epoch 84 | Train MSE: 0.050591 | Val MSE: 0.050581 | Test MSE: 0.050792\n",
      "Epoch 85 | Train MSE: 0.050539 | Val MSE: 0.049399 | Test MSE: 0.049684\n",
      "Epoch 86 | Train MSE: 0.050476 | Val MSE: 0.049112 | Test MSE: 0.049440\n",
      "Epoch 87 | Train MSE: 0.050286 | Val MSE: 0.049827 | Test MSE: 0.050090\n",
      "Epoch 88 | Train MSE: 0.050275 | Val MSE: 0.049923 | Test MSE: 0.050240\n",
      "Epoch 89 | Train MSE: 0.050219 | Val MSE: 0.049410 | Test MSE: 0.049703\n",
      "Epoch 90 | Train MSE: 0.050321 | Val MSE: 0.049144 | Test MSE: 0.049445\n",
      "Epoch 91 | Train MSE: 0.049930 | Val MSE: 0.048874 | Test MSE: 0.049207\n",
      "Epoch 92 | Train MSE: 0.049907 | Val MSE: 0.049875 | Test MSE: 0.050121\n",
      "Models saved successfully.\n",
      "Epoch 93 | Train MSE: 0.049889 | Val MSE: 0.048399 | Test MSE: 0.048760\n",
      "Epoch 94 | Train MSE: 0.049812 | Val MSE: 0.049846 | Test MSE: 0.050288\n",
      "Models saved successfully.\n",
      "Epoch 95 | Train MSE: 0.049773 | Val MSE: 0.048274 | Test MSE: 0.048595\n",
      "Epoch 96 | Train MSE: 0.049679 | Val MSE: 0.050115 | Test MSE: 0.050431\n",
      "Epoch 97 | Train MSE: 0.049790 | Val MSE: 0.049505 | Test MSE: 0.049853\n",
      "Epoch 98 | Train MSE: 0.049588 | Val MSE: 0.051209 | Test MSE: 0.051508\n",
      "Models saved successfully.\n",
      "Epoch 99 | Train MSE: 0.049541 | Val MSE: 0.048200 | Test MSE: 0.048577\n",
      "Epoch 100 | Train MSE: 0.049394 | Val MSE: 0.048903 | Test MSE: 0.049228\n"
     ]
    }
   ],
   "source": [
    "seq_len = X_train.shape[1]\n",
    "num_features = X_train.shape[2]\n",
    "\n",
    "# Parameters\n",
    "B, T, D = batch_size, seq_len, num_features\n",
    "emb_dim = 64\n",
    "channels = [32, 64, 128]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate model\n",
    "model = TCNAutoencoder(input_dim=D, emb_dim=emb_dim, seq_len=T, channels=channels).to(device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "params = list(model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "best_loss = 0.050461 + 0.050724\n",
    "\n",
    "encoder_path = \"/kaggle/working/encoder.pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(encoder_path))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (x,) in loader:\n",
    "            x = x.to(device)\n",
    "            x_hat = model(x)\n",
    "            loss = loss_fn(x_hat, x)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(71, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for (x,) in train_loader:\n",
    "        x = x.to(device)\n",
    "        x_hat = model(x)\n",
    "        loss = loss_fn(x_hat, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    test_loss = evaluate(model, test_loader)\n",
    "\n",
    "    if val_loss + test_loss < best_loss:\n",
    "        best_loss = val_loss + test_loss\n",
    "        # Saves the weights\n",
    "        torch.save(model.state_dict(), encoder_path)\n",
    "        print(\"Models saved successfully.\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f} | Test MSE: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f00e722-b355-4a83-b07d-e24363c8f55a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:30:20.199970Z",
     "iopub.status.busy": "2025-09-13T12:30:20.199674Z",
     "iopub.status.idle": "2025-09-13T12:41:03.053221Z",
     "shell.execute_reply": "2025-09-13T12:41:03.052471Z",
     "shell.execute_reply.started": "2025-09-13T12:30:20.199952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101 | Train MSE: 0.049223 | Val MSE: 0.049476 | Test MSE: 0.049799\n",
      "Epoch 102 | Train MSE: 0.049452 | Val MSE: 0.048634 | Test MSE: 0.049064\n",
      "Models saved successfully.\n",
      "Epoch 103 | Train MSE: 0.049170 | Val MSE: 0.048099 | Test MSE: 0.048386\n",
      "Epoch 104 | Train MSE: 0.049068 | Val MSE: 0.048749 | Test MSE: 0.049118\n",
      "Models saved successfully.\n",
      "Epoch 105 | Train MSE: 0.049031 | Val MSE: 0.047883 | Test MSE: 0.048154\n",
      "Epoch 106 | Train MSE: 0.049005 | Val MSE: 0.048917 | Test MSE: 0.049332\n",
      "Epoch 107 | Train MSE: 0.049287 | Val MSE: 0.048663 | Test MSE: 0.048924\n",
      "Epoch 108 | Train MSE: 0.048993 | Val MSE: 0.048572 | Test MSE: 0.048877\n",
      "Epoch 109 | Train MSE: 0.048697 | Val MSE: 0.048175 | Test MSE: 0.048463\n",
      "Epoch 110 | Train MSE: 0.048796 | Val MSE: 0.049168 | Test MSE: 0.049459\n",
      "Epoch 111 | Train MSE: 0.048809 | Val MSE: 0.048136 | Test MSE: 0.048329\n",
      "Epoch 112 | Train MSE: 0.048786 | Val MSE: 0.048288 | Test MSE: 0.048554\n",
      "Models saved successfully.\n",
      "Epoch 113 | Train MSE: 0.048670 | Val MSE: 0.047679 | Test MSE: 0.048029\n",
      "Epoch 114 | Train MSE: 0.048678 | Val MSE: 0.049307 | Test MSE: 0.049657\n",
      "Epoch 115 | Train MSE: 0.048540 | Val MSE: 0.048074 | Test MSE: 0.048396\n",
      "Models saved successfully.\n",
      "Epoch 116 | Train MSE: 0.048549 | Val MSE: 0.047522 | Test MSE: 0.047786\n",
      "Epoch 117 | Train MSE: 0.048304 | Val MSE: 0.048024 | Test MSE: 0.048160\n",
      "Models saved successfully.\n",
      "Epoch 118 | Train MSE: 0.048418 | Val MSE: 0.047423 | Test MSE: 0.047768\n",
      "Epoch 119 | Train MSE: 0.048443 | Val MSE: 0.047709 | Test MSE: 0.048073\n",
      "Epoch 120 | Train MSE: 0.048350 | Val MSE: 0.048726 | Test MSE: 0.049631\n",
      "Epoch 121 | Train MSE: 0.048346 | Val MSE: 0.048657 | Test MSE: 0.048809\n",
      "Epoch 122 | Train MSE: 0.048207 | Val MSE: 0.050140 | Test MSE: 0.050550\n",
      "Models saved successfully.\n",
      "Epoch 123 | Train MSE: 0.048308 | Val MSE: 0.047369 | Test MSE: 0.047671\n",
      "Epoch 124 | Train MSE: 0.048025 | Val MSE: 0.048541 | Test MSE: 0.049021\n",
      "Epoch 125 | Train MSE: 0.048481 | Val MSE: 0.047701 | Test MSE: 0.047984\n",
      "Epoch 126 | Train MSE: 0.048006 | Val MSE: 0.048659 | Test MSE: 0.048850\n",
      "Epoch 127 | Train MSE: 0.048133 | Val MSE: 0.048406 | Test MSE: 0.048602\n",
      "Epoch 128 | Train MSE: 0.048017 | Val MSE: 0.047505 | Test MSE: 0.047665\n",
      "Epoch 129 | Train MSE: 0.047969 | Val MSE: 0.047845 | Test MSE: 0.048047\n",
      "Epoch 130 | Train MSE: 0.048172 | Val MSE: 0.051160 | Test MSE: 0.051675\n",
      "Epoch 131 | Train MSE: 0.047953 | Val MSE: 0.048315 | Test MSE: 0.048519\n",
      "Models saved successfully.\n",
      "Epoch 132 | Train MSE: 0.047763 | Val MSE: 0.046755 | Test MSE: 0.047122\n",
      "Epoch 133 | Train MSE: 0.047773 | Val MSE: 0.047927 | Test MSE: 0.048304\n",
      "Epoch 134 | Train MSE: 0.047774 | Val MSE: 0.047933 | Test MSE: 0.048246\n",
      "Epoch 135 | Train MSE: 0.047687 | Val MSE: 0.047301 | Test MSE: 0.047623\n",
      "Epoch 136 | Train MSE: 0.047802 | Val MSE: 0.047250 | Test MSE: 0.047551\n",
      "Epoch 137 | Train MSE: 0.047603 | Val MSE: 0.048761 | Test MSE: 0.049029\n",
      "Epoch 138 | Train MSE: 0.047834 | Val MSE: 0.048150 | Test MSE: 0.048394\n",
      "Epoch 139 | Train MSE: 0.047524 | Val MSE: 0.049393 | Test MSE: 0.049647\n",
      "Epoch 140 | Train MSE: 0.047436 | Val MSE: 0.048514 | Test MSE: 0.048763\n",
      "Epoch 141 | Train MSE: 0.047564 | Val MSE: 0.047726 | Test MSE: 0.048178\n",
      "Epoch 142 | Train MSE: 0.047485 | Val MSE: 0.048494 | Test MSE: 0.048896\n",
      "Epoch 143 | Train MSE: 0.047483 | Val MSE: 0.048169 | Test MSE: 0.048403\n",
      "Epoch 144 | Train MSE: 0.047388 | Val MSE: 0.048088 | Test MSE: 0.048424\n",
      "Epoch 145 | Train MSE: 0.047320 | Val MSE: 0.047087 | Test MSE: 0.047463\n",
      "Epoch 146 | Train MSE: 0.047401 | Val MSE: 0.048230 | Test MSE: 0.048417\n",
      "Epoch 147 | Train MSE: 0.047380 | Val MSE: 0.047347 | Test MSE: 0.047663\n",
      "Epoch 148 | Train MSE: 0.047485 | Val MSE: 0.049113 | Test MSE: 0.049320\n",
      "Epoch 149 | Train MSE: 0.047290 | Val MSE: 0.047658 | Test MSE: 0.047950\n",
      "Epoch 150 | Train MSE: 0.047237 | Val MSE: 0.046879 | Test MSE: 0.047148\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(101, 151):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for (x,) in train_loader:\n",
    "        x = x.to(device)\n",
    "        x_hat = model(x)\n",
    "        loss = loss_fn(x_hat, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    test_loss = evaluate(model, test_loader)\n",
    "\n",
    "    if val_loss + test_loss < best_loss:\n",
    "        best_loss = val_loss + test_loss\n",
    "        # Saves the weights\n",
    "        torch.save(model.state_dict(), encoder_path)\n",
    "        print(\"Models saved successfully.\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f} | Test MSE: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f3cc181-c113-4a1e-be3d-00b36128954b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:41:03.054596Z",
     "iopub.status.busy": "2025-09-13T12:41:03.054366Z",
     "iopub.status.idle": "2025-09-13T12:51:47.113205Z",
     "shell.execute_reply": "2025-09-13T12:51:47.112505Z",
     "shell.execute_reply.started": "2025-09-13T12:41:03.054579Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151 | Train MSE: 0.047142 | Val MSE: 0.047605 | Test MSE: 0.047886\n",
      "Epoch 152 | Train MSE: 0.047072 | Val MSE: 0.049000 | Test MSE: 0.049240\n",
      "Epoch 153 | Train MSE: 0.047333 | Val MSE: 0.047327 | Test MSE: 0.047681\n",
      "Epoch 154 | Train MSE: 0.047107 | Val MSE: 0.048899 | Test MSE: 0.049177\n",
      "Epoch 155 | Train MSE: 0.047065 | Val MSE: 0.047188 | Test MSE: 0.047434\n",
      "Epoch 156 | Train MSE: 0.046981 | Val MSE: 0.047257 | Test MSE: 0.047553\n",
      "Epoch 157 | Train MSE: 0.046996 | Val MSE: 0.047498 | Test MSE: 0.047841\n",
      "Epoch 158 | Train MSE: 0.046940 | Val MSE: 0.046780 | Test MSE: 0.047223\n",
      "Epoch 159 | Train MSE: 0.046924 | Val MSE: 0.047530 | Test MSE: 0.047834\n",
      "Epoch 160 | Train MSE: 0.047026 | Val MSE: 0.048315 | Test MSE: 0.048634\n",
      "Epoch 161 | Train MSE: 0.046927 | Val MSE: 0.047648 | Test MSE: 0.047866\n",
      "Epoch 162 | Train MSE: 0.046843 | Val MSE: 0.047264 | Test MSE: 0.047583\n",
      "Epoch 163 | Train MSE: 0.046834 | Val MSE: 0.048008 | Test MSE: 0.048276\n",
      "Epoch 164 | Train MSE: 0.046772 | Val MSE: 0.047933 | Test MSE: 0.048148\n",
      "Epoch 165 | Train MSE: 0.046733 | Val MSE: 0.046908 | Test MSE: 0.047248\n",
      "Epoch 166 | Train MSE: 0.046761 | Val MSE: 0.048166 | Test MSE: 0.048317\n",
      "Epoch 167 | Train MSE: 0.046752 | Val MSE: 0.047673 | Test MSE: 0.047979\n",
      "Epoch 168 | Train MSE: 0.046757 | Val MSE: 0.047720 | Test MSE: 0.047975\n",
      "Epoch 169 | Train MSE: 0.046716 | Val MSE: 0.047232 | Test MSE: 0.047556\n",
      "Epoch 170 | Train MSE: 0.046647 | Val MSE: 0.047420 | Test MSE: 0.047710\n",
      "Epoch 171 | Train MSE: 0.046651 | Val MSE: 0.048499 | Test MSE: 0.048704\n",
      "Models saved successfully.\n",
      "Epoch 172 | Train MSE: 0.046662 | Val MSE: 0.046368 | Test MSE: 0.046626\n",
      "Epoch 173 | Train MSE: 0.046567 | Val MSE: 0.047234 | Test MSE: 0.047428\n",
      "Epoch 174 | Train MSE: 0.046549 | Val MSE: 0.046811 | Test MSE: 0.047050\n",
      "Epoch 175 | Train MSE: 0.046608 | Val MSE: 0.046596 | Test MSE: 0.046919\n",
      "Epoch 176 | Train MSE: 0.046480 | Val MSE: 0.047329 | Test MSE: 0.047605\n",
      "Epoch 177 | Train MSE: 0.046619 | Val MSE: 0.046530 | Test MSE: 0.046813\n",
      "Models saved successfully.\n",
      "Epoch 178 | Train MSE: 0.046407 | Val MSE: 0.045739 | Test MSE: 0.046030\n",
      "Epoch 179 | Train MSE: 0.046392 | Val MSE: 0.046624 | Test MSE: 0.046922\n",
      "Epoch 180 | Train MSE: 0.046339 | Val MSE: 0.046891 | Test MSE: 0.047173\n",
      "Epoch 181 | Train MSE: 0.046482 | Val MSE: 0.047640 | Test MSE: 0.047925\n",
      "Epoch 182 | Train MSE: 0.046406 | Val MSE: 0.048554 | Test MSE: 0.048916\n",
      "Epoch 183 | Train MSE: 0.046442 | Val MSE: 0.047179 | Test MSE: 0.047393\n",
      "Epoch 184 | Train MSE: 0.046219 | Val MSE: 0.047350 | Test MSE: 0.047714\n",
      "Epoch 185 | Train MSE: 0.046294 | Val MSE: 0.046678 | Test MSE: 0.046783\n",
      "Epoch 186 | Train MSE: 0.046347 | Val MSE: 0.047467 | Test MSE: 0.047738\n",
      "Epoch 187 | Train MSE: 0.046212 | Val MSE: 0.047291 | Test MSE: 0.047708\n",
      "Epoch 188 | Train MSE: 0.046245 | Val MSE: 0.047545 | Test MSE: 0.047723\n",
      "Epoch 189 | Train MSE: 0.046140 | Val MSE: 0.047234 | Test MSE: 0.047540\n",
      "Epoch 190 | Train MSE: 0.046609 | Val MSE: 0.047257 | Test MSE: 0.047505\n",
      "Epoch 191 | Train MSE: 0.046063 | Val MSE: 0.046927 | Test MSE: 0.047237\n",
      "Epoch 192 | Train MSE: 0.046108 | Val MSE: 0.047259 | Test MSE: 0.047482\n",
      "Epoch 193 | Train MSE: 0.046230 | Val MSE: 0.046693 | Test MSE: 0.046867\n",
      "Epoch 194 | Train MSE: 0.046203 | Val MSE: 0.046351 | Test MSE: 0.046604\n",
      "Epoch 195 | Train MSE: 0.046018 | Val MSE: 0.048227 | Test MSE: 0.048348\n",
      "Epoch 196 | Train MSE: 0.046110 | Val MSE: 0.047093 | Test MSE: 0.047368\n",
      "Epoch 197 | Train MSE: 0.045950 | Val MSE: 0.047768 | Test MSE: 0.047976\n",
      "Epoch 198 | Train MSE: 0.046008 | Val MSE: 0.047464 | Test MSE: 0.047587\n",
      "Epoch 199 | Train MSE: 0.046035 | Val MSE: 0.048085 | Test MSE: 0.048411\n",
      "Epoch 200 | Train MSE: 0.046102 | Val MSE: 0.046243 | Test MSE: 0.046488\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(151, 201):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for (x,) in train_loader:\n",
    "        x = x.to(device)\n",
    "        x_hat = model(x)\n",
    "        loss = loss_fn(x_hat, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    test_loss = evaluate(model, test_loader)\n",
    "\n",
    "    if val_loss + test_loss < best_loss:\n",
    "        best_loss = val_loss + test_loss\n",
    "        # Saves the weights\n",
    "        torch.save(model.state_dict(), encoder_path)\n",
    "        print(\"Models saved successfully.\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f} | Test MSE: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8499e3",
   "metadata": {},
   "source": [
    "# Save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "446e901f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:51:47.114277Z",
     "iopub.status.busy": "2025-09-13T12:51:47.114075Z",
     "iopub.status.idle": "2025-09-13T12:51:47.153136Z",
     "shell.execute_reply": "2025-09-13T12:51:47.152555Z",
     "shell.execute_reply.started": "2025-09-13T12:51:47.114256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models reloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_loaded = TCNAutoencoder(input_dim=D, emb_dim=emb_dim, seq_len=T, channels=channels)\n",
    "\n",
    "model_loaded.load_state_dict(torch.load(encoder_path))\n",
    "\n",
    "model_loaded.to(device)\n",
    "\n",
    "model_loaded.eval()\n",
    "\n",
    "print(\"Models reloaded successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8257006,
     "sourceId": 13039715,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
