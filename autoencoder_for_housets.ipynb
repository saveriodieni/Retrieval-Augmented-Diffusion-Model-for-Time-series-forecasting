{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12300120,"sourceType":"datasetVersion","datasetId":7752660}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"7dc1d880","cell_type":"markdown","source":"# Imports","metadata":{}},{"id":"be2f0f9a","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport os\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:16.337219Z","iopub.execute_input":"2025-06-27T08:43:16.337410Z","iopub.status.idle":"2025-06-27T08:43:24.560587Z","shell.execute_reply.started":"2025-06-27T08:43:16.337393Z","shell.execute_reply":"2025-06-27T08:43:24.560003Z"}},"outputs":[],"execution_count":1},{"id":"3a740f43","cell_type":"markdown","source":"# Data extraction and preprocessing","metadata":{}},{"id":"6a717c9f","cell_type":"code","source":"# Carica il CSV\ncsv_file = \"/kaggle/input/housets/HouseTS_with_images.csv\"\ndf = pd.read_csv(csv_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:24.561660Z","iopub.execute_input":"2025-06-27T08:43:24.561960Z","iopub.status.idle":"2025-06-27T08:43:24.996363Z","shell.execute_reply.started":"2025-06-27T08:43:24.561941Z","shell.execute_reply":"2025-06-27T08:43:24.995808Z"}},"outputs":[],"execution_count":2},{"id":"ff7912ff","cell_type":"code","source":"# Conta quante righe ci sono per ciascun zipcode\ncount_per_zipcode = df[\"zipcode\"].value_counts()\n\n# Calcola la media\naverage_count = count_per_zipcode.mean()\n\nprint(f\"Numero medio di entries per zipcode: {average_count:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:24.997076Z","iopub.execute_input":"2025-06-27T08:43:24.997322Z","iopub.status.idle":"2025-06-27T08:43:25.018089Z","shell.execute_reply.started":"2025-06-27T08:43:24.997300Z","shell.execute_reply":"2025-06-27T08:43:25.017540Z"}},"outputs":[{"name":"stdout","text":"Numero medio di entries per zipcode: 142.00\n","output_type":"stream"}],"execution_count":3},{"id":"0d964f5a","cell_type":"code","source":"# Assicurati che la colonna 'date' sia in formato datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Filtra per lo zipcode desiderato\nzipcode = 20001\ndf_zip = df[df['zipcode'] == zipcode].sort_values('date')\n\n# Calcola le differenze di tempo tra date consecutive\ntime_diffs = df_zip['date'].diff().dropna()\n\n# Calcola il timestep medio (in giorni)\naverage_timestep = time_diffs.mean()\n\nprint(f\"Timestep medio per zipcode {zipcode}: {average_timestep}\")\nprint(f\"In giorni: {average_timestep.days} giorni\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:25.018659Z","iopub.execute_input":"2025-06-27T08:43:25.018821Z","iopub.status.idle":"2025-06-27T08:43:25.070192Z","shell.execute_reply.started":"2025-06-27T08:43:25.018807Z","shell.execute_reply":"2025-06-27T08:43:25.069680Z"}},"outputs":[{"name":"stdout","text":"Timestep medio per zipcode 20001: 30 days 10:33:11.489361702\nIn giorni: 30 giorni\n","output_type":"stream"}],"execution_count":4},{"id":"888664ce","cell_type":"code","source":"# Seleziona le colonne che non sono di tipo numerico (int, float)\nnon_numeric_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n\nprint(\"Colonne non numeriche nel dataset:\")\nfor col in non_numeric_cols:\n    print(col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:25.071742Z","iopub.execute_input":"2025-06-27T08:43:25.071947Z","iopub.status.idle":"2025-06-27T08:43:25.077199Z","shell.execute_reply.started":"2025-06-27T08:43:25.071932Z","shell.execute_reply":"2025-06-27T08:43:25.076598Z"}},"outputs":[{"name":"stdout","text":"Colonne non numeriche nel dataset:\ndate\ncity\ncity_full\n","output_type":"stream"}],"execution_count":5},{"id":"d08abc28","cell_type":"code","source":"unique_cities = df['city_full'].unique()\n\nprint(\"Valori unici nella colonna 'city':\")\nfor city in unique_cities:\n    print(city)\n\ndf = df.drop(columns=['city', 'city_full'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:25.077897Z","iopub.execute_input":"2025-06-27T08:43:25.078133Z","iopub.status.idle":"2025-06-27T08:43:25.103070Z","shell.execute_reply.started":"2025-06-27T08:43:25.078115Z","shell.execute_reply":"2025-06-27T08:43:25.102349Z"}},"outputs":[{"name":"stdout","text":"Valori unici nella colonna 'city':\nDC_Metro\n","output_type":"stream"}],"execution_count":6},{"id":"fc5d1845","cell_type":"code","source":"window_length = 12\nstep_size = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:25.103767Z","iopub.execute_input":"2025-06-27T08:43:25.103956Z","iopub.status.idle":"2025-06-27T08:43:25.107563Z","shell.execute_reply.started":"2025-06-27T08:43:25.103941Z","shell.execute_reply":"2025-06-27T08:43:25.106825Z"}},"outputs":[],"execution_count":7},{"id":"8e3db128","cell_type":"code","source":"df['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(['zipcode', 'date']).reset_index(drop=True)\n\nall_windows = []\nall_meta = []\n\ndef extract_windows(df_sub, window_len, step):\n    windows = []\n    for i in range(0, len(df_sub) - window_len + 1, step):\n        window = df_sub.iloc[i:i+window_len].drop(columns=['date', 'zipcode', 'year']).values\n        windows.append(window)\n    return np.array(windows)\n\n# Raggruppa per zipcode\nfor zipcode, group in df.groupby('zipcode'):\n    group = group.reset_index(drop=True)\n    if len(group) < window_length:\n        continue\n\n    windows = extract_windows(group, window_length, step_size)\n    all_windows.append(windows)\n\n    # Salva metadati: per esempio la data di inizio e lo zipcode della finestra\n    window_meta = [(zipcode, group.loc[i, 'date'], group.loc[i, 'year']) for i in range(len(group) - window_length + 1)]\n    all_meta.extend(window_meta)\n\n# Concateno tutte le finestre\nX = np.vstack(all_windows)  # shape: (num_samples, window_length, num_features senza date/zipcode)\n\nprint(\"Shape of X:\", X.shape)\nprint(\"Number of metadata entries:\", len(all_meta))\n\n# Esempio di stampa primo meta\nprint(\"Esempio metadato finestra 0:\", all_meta[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:25.108328Z","iopub.execute_input":"2025-06-27T08:43:25.108579Z","iopub.status.idle":"2025-06-27T08:43:36.353386Z","shell.execute_reply.started":"2025-06-27T08:43:25.108562Z","shell.execute_reply":"2025-06-27T08:43:36.352561Z"}},"outputs":[{"name":"stdout","text":"Shape of X: (40348, 12, 34)\nNumber of metadata entries: 40348\nEsempio metadato finestra 0: (20001, Timestamp('2012-03-31 00:00:00'), 2012)\n","output_type":"stream"}],"execution_count":8},{"id":"7e3e4b15","cell_type":"code","source":"from sklearn.utils import shuffle\n\nX, window_ids = shuffle(X, all_meta, random_state=42)\n\ntrain_size = int(0.7 * len(X))\nval_size = int(0.1 * len(X))\ntest_size = len(X) - train_size - val_size\n\nX_train = X[:train_size]\nX_val = X[train_size:train_size + val_size]\nX_test = X[train_size + val_size:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:36.354288Z","iopub.execute_input":"2025-06-27T08:43:36.354607Z","iopub.status.idle":"2025-06-27T08:43:36.802316Z","shell.execute_reply.started":"2025-06-27T08:43:36.354580Z","shell.execute_reply":"2025-06-27T08:43:36.801534Z"}},"outputs":[],"execution_count":9},{"id":"661cfd8b","cell_type":"markdown","source":"# Encoder and Decoder","metadata":{}},{"id":"6359a70b","cell_type":"markdown","source":"Our goal was to build an encoder-decoder model able to learn a compressed representaion of the input time series, so to allow a more efficient search of similar time series in a smaller dimensional space, speeding up the task of finding k nearest neighbours. </br> The encoder gets as input a tensor of shape (batch_size, seq_len, num_features) and compresses it into a tensor of shape (batch_size, embedding_dim), while the decoder takes the output of the encoder and tries to reconstruct the original input. ","metadata":{}},{"id":"1a9a7560","cell_type":"code","source":"class Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super().__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size] if self.chomp_size > 0 else x\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout):\n        super().__init__()\n        padding = (kernel_size - 1) * dilation  # full causal\n        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n                               padding=padding, dilation=dilation)\n        self.chomp1 = Chomp1d(padding)\n\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n                               padding=padding, dilation=dilation)\n        self.chomp2 = Chomp1d(padding)\n\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(\n            self.conv1, self.chomp1, self.relu1, self.dropout1,\n            self.conv2, self.chomp2, self.relu2, self.dropout2,\n        )\n\n        self.downsample = nn.Conv1d(in_channels, out_channels, 1) \\\n            if in_channels != out_channels else None\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        res = x if self.downsample is None else self.downsample(x)\n        out = self.net(x)\n        if out.shape != res.shape:\n            # Align time dimension by cropping the residual (this might be necessary in some edge cases)\n            min_len = min(out.size(-1), res.size(-1))\n            out = out[..., :min_len]\n            res = res[..., :min_len]\n        return self.relu(out + res)\n\n# Encoder\nclass TCNEncoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, num_channels, kernel_size=3, dropout=0.2):\n        super().__init__()\n        layers = []\n        for i in range(len(num_channels)):\n            in_ch = input_dim if i == 0 else num_channels[i - 1]\n            out_ch = num_channels[i]\n            dilation = 2 ** i\n            layers.append(TemporalBlock(in_ch, out_ch, kernel_size, dilation, dropout))\n        self.tcn = nn.Sequential(*layers)\n\n        # Projection from [B, C, T] to [B, emb_dim]\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.project = nn.Linear(num_channels[-1], emb_dim)\n\n    def forward(self, x):\n        # x: [B, T, D] → [B, D, T]\n        x = x.permute(0, 2, 1)\n        x = self.tcn(x)  # [B, C, T]\n        x = self.pool(x).squeeze(-1)  # [B, C]\n        x = self.project(x)  # [B, emb_dim]\n        return x\n\n# Decoder\nclass TCNDecoder(nn.Module):\n    def __init__(self, emb_dim, output_dim, seq_len, num_channels, kernel_size=3, dropout=0.2):\n        super().__init__()\n        self.seq_len = seq_len\n        self.output_dim = output_dim\n\n        # Project embedding back to a sequence shape: [B, C, T]\n        self.expand = nn.Linear(emb_dim, num_channels[0] * seq_len)\n\n        layers = []\n        for i in range(len(num_channels) - 1):\n            in_ch = num_channels[i]\n            out_ch = num_channels[i + 1]\n            dilation = 2 ** i\n            layers.append(TemporalBlock(in_ch, out_ch, kernel_size, dilation, dropout))\n\n        self.tcn = nn.Sequential(*layers)\n        self.out_proj = nn.Conv1d(num_channels[-1], output_dim, kernel_size=1)\n\n    def forward(self, x):\n        # x: [B, emb_dim] → [B, C0, T]\n        x = self.expand(x)  # [B, C0 * T]\n        x = x.view(x.size(0), -1, self.seq_len)  # [B, C0, T]\n        x = self.tcn(x)  # [B, Cn, T]\n        x = self.out_proj(x)  # [B, D, T]\n        return x.permute(0, 2, 1)  # [B, T, D]\n    \nclass TCNAutoencoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, seq_len, channels, kernel_size=3, dropout=0.2):\n        super().__init__()\n        self.encoder = TCNEncoder(input_dim, emb_dim, channels, kernel_size, dropout)\n        self.decoder = TCNDecoder(emb_dim, input_dim, seq_len, channels[::-1], kernel_size, dropout)\n\n    def forward(self, x, only_encoder = False):\n        # x: [B, T, D]\n        z = self.encoder(x)       # [B, emb_dim]\n        if only_encoder:\n            return z\n        x_recon = self.decoder(z) # [B, T, D]\n        return x_recon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:36.803311Z","iopub.execute_input":"2025-06-27T08:43:36.803692Z","iopub.status.idle":"2025-06-27T08:43:36.820937Z","shell.execute_reply.started":"2025-06-27T08:43:36.803670Z","shell.execute_reply":"2025-06-27T08:43:36.820372Z"}},"outputs":[],"execution_count":10},{"id":"d42669af","cell_type":"code","source":"# Compute the statistics of the train dataset and normalize with respect to it \n\nmean = X_train.mean(axis=(0, 1), keepdims=True)  # shape (1, 1, num_features)\nstd = X_train.std(axis=(0, 1), keepdims=True)\n\nX_train_norm = (X_train - mean) / std\nX_val_norm = (X_val - mean) / std\nX_test_norm = (X_test - mean) / std\n\nprint(mean, std)\n\nmean = X_train_norm.mean(axis=(0, 1), keepdims=True)  # shape (1, 1, num_features)\nstd = X_train_norm.std(axis=(0, 1), keepdims=True)\n\nprint(mean, std)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:36.821647Z","iopub.execute_input":"2025-06-27T08:43:36.821897Z","iopub.status.idle":"2025-06-27T08:43:37.109376Z","shell.execute_reply.started":"2025-06-27T08:43:36.821871Z","shell.execute_reply":"2025-06-27T08:43:37.108733Z"}},"outputs":[{"name":"stdout","text":"[[[4.54852826e+05 5.58716962e+05 2.27982484e+02 2.46098550e+02\n   6.83864232e+01 7.45339848e+01 8.01239924e+01 4.91713935e+01\n   5.41263056e+01 9.88508572e-01 2.73734409e-01 3.22142625e-01\n   2.36917260e+01 7.18886096e-01 2.33493255e+00 1.18643558e+00\n   8.91169936e+01 9.04543250e+01 5.97777915e+01 4.15741364e+00\n   1.16968157e+01 1.90285746e+04 3.99743190e+01 4.78702971e+04\n   1.87499750e+04 7.36403461e+03 1.59877327e+03 4.40833370e+05\n   1.08915696e+04 6.19120623e+02 1.82830710e+04 1.82830710e+04\n   9.33441450e+03 4.57644868e+05]]] [[[2.47671163e+05 8.77060063e+06 1.15036342e+02 5.82713229e+02\n   7.01868096e+01 7.54677556e+01 8.32473874e+01 4.95942947e+01\n   7.83625340e+01 3.22186448e-02 1.94536395e-01 2.48884578e-01\n   4.77475403e+01 1.64222823e+00 4.26626174e+00 1.79608954e+00\n   1.57134652e+02 1.90464107e+02 8.16188975e+01 8.63331047e+00\n   1.59101982e+01 1.75557344e+04 7.11650516e+00 1.92278578e+04\n   1.73431754e+04 6.86866352e+03 4.32730094e+02 1.83839663e+05\n   1.01493864e+04 7.54798540e+02 1.68373952e+04 1.68373952e+04\n   8.72958115e+03 2.01021908e+05]]]\n[[[ 1.04901885e-15 -1.83335446e-16 -6.88877990e-14 -1.39840261e-14\n   -6.03017553e-16 -1.14152231e-14 -6.21827718e-16 -6.56158331e-16\n   -2.27563683e-17  2.87788068e-13  6.17798562e-13  2.99696542e-13\n    3.46190048e-15 -2.62387543e-14 -5.26354523e-15 -6.06668111e-15\n   -1.31158943e-15  6.39401535e-15  6.03822746e-17  7.73458912e-15\n   -1.10211213e-16  7.42377132e-16 -7.47775551e-13 -7.42611270e-17\n    7.41233794e-17  6.64851009e-17  3.82536899e-16  7.23618967e-17\n   -1.50980623e-16  5.57425705e-16 -1.70948585e-16 -1.70948585e-16\n   -3.01865348e-16 -4.81838434e-15]]] [[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n   1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]\n","output_type":"stream"}],"execution_count":11},{"id":"8bdd7eea","cell_type":"code","source":"# Convert into PyTorch tensors\nX_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\nX_val_tensor = torch.tensor(X_val_norm, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test_norm, dtype=torch.float32)\n\n# Dataset & DataLoader\nbatch_size = 32\ntrain_loader = DataLoader(TensorDataset(X_train_tensor), batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(TensorDataset(X_val_tensor), batch_size=batch_size)\ntest_loader = DataLoader(TensorDataset(X_test_tensor), batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:37.110152Z","iopub.execute_input":"2025-06-27T08:43:37.110384Z","iopub.status.idle":"2025-06-27T08:43:37.210507Z","shell.execute_reply.started":"2025-06-27T08:43:37.110366Z","shell.execute_reply":"2025-06-27T08:43:37.209939Z"}},"outputs":[],"execution_count":12},{"id":"ec739f61","cell_type":"markdown","source":"# Model training","metadata":{}},{"id":"dac80033","cell_type":"code","source":"seq_len = X_train.shape[1]\nnum_features = X_train.shape[2]\n\n# Parameters\nB, T, D = batch_size, seq_len, num_features\nemb_dim = 64\nchannels = [32, 64, 128]\n\n# Instantiate model\nmodel = TCNAutoencoder(input_dim=D, emb_dim=emb_dim, seq_len=T, channels=channels).to(device)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Model parameters: {count_parameters(model):,}\")\n\nparams = list(model.parameters())\n\noptimizer = torch.optim.Adam(params, lr=1e-3)\nloss_fn = nn.MSELoss()\n\nepochs = 100\n\nbest_loss = 1e10\n\nencoder_path = \"/kaggle/working/encoder.pth\"\n\ndef evaluate(model, loader):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for (x,) in loader:\n            x = x.to(device)\n            x_hat = model(x)\n            loss = loss_fn(x_hat, x)\n            total_loss += loss.item()\n    return total_loss / len(loader)\n\nfor epoch in range(1, epochs + 1):\n    model.train()\n    train_loss = 0\n\n    for (x,) in train_loader:\n        x = x.to(device)\n        x_hat = model(x)\n        loss = loss_fn(x_hat, x)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n    val_loss = evaluate(model, val_loader)\n    test_loss = evaluate(model, test_loader)\n\n    if val_loss + test_loss < best_loss:\n        best_loss = val_loss + test_loss\n        # Saves the weights\n        torch.save(model.state_dict(), encoder_path)\n        print(\"Models saved successfully.\")\n\n    print(f\"Epoch {epoch:02d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f} | Test MSE: {test_loss:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:44:38.254702Z","iopub.execute_input":"2025-06-27T08:44:38.254987Z","iopub.status.idle":"2025-06-27T08:57:05.875341Z","shell.execute_reply.started":"2025-06-27T08:44:38.254968Z","shell.execute_reply":"2025-06-27T08:57:05.874701Z"}},"outputs":[{"name":"stdout","text":"Model parameters: 276,322\nModels saved successfully.\nEpoch 01 | Train MSE: 0.329717 | Val MSE: 0.194305 | Test MSE: 0.226819\nModels saved successfully.\nEpoch 02 | Train MSE: 0.211920 | Val MSE: 0.149754 | Test MSE: 0.189440\nModels saved successfully.\nEpoch 03 | Train MSE: 0.182255 | Val MSE: 0.138717 | Test MSE: 0.172429\nModels saved successfully.\nEpoch 04 | Train MSE: 0.159782 | Val MSE: 0.105315 | Test MSE: 0.146662\nEpoch 05 | Train MSE: 0.141739 | Val MSE: 0.111816 | Test MSE: 0.143787\nModels saved successfully.\nEpoch 06 | Train MSE: 0.141896 | Val MSE: 0.099963 | Test MSE: 0.134545\nModels saved successfully.\nEpoch 07 | Train MSE: 0.132991 | Val MSE: 0.098059 | Test MSE: 0.122779\nModels saved successfully.\nEpoch 08 | Train MSE: 0.124939 | Val MSE: 0.088298 | Test MSE: 0.110709\nModels saved successfully.\nEpoch 09 | Train MSE: 0.125151 | Val MSE: 0.085099 | Test MSE: 0.110870\nModels saved successfully.\nEpoch 10 | Train MSE: 0.115700 | Val MSE: 0.081556 | Test MSE: 0.105681\nEpoch 11 | Train MSE: 0.109634 | Val MSE: 0.081358 | Test MSE: 0.113280\nModels saved successfully.\nEpoch 12 | Train MSE: 0.108042 | Val MSE: 0.074726 | Test MSE: 0.095963\nEpoch 13 | Train MSE: 0.099981 | Val MSE: 0.075216 | Test MSE: 0.108042\nEpoch 14 | Train MSE: 0.101760 | Val MSE: 0.078464 | Test MSE: 0.096210\nEpoch 15 | Train MSE: 0.098398 | Val MSE: 0.078726 | Test MSE: 0.093063\nModels saved successfully.\nEpoch 16 | Train MSE: 0.094490 | Val MSE: 0.065578 | Test MSE: 0.091645\nEpoch 17 | Train MSE: 0.091457 | Val MSE: 0.076650 | Test MSE: 0.091379\nEpoch 18 | Train MSE: 0.094312 | Val MSE: 0.070610 | Test MSE: 0.097959\nModels saved successfully.\nEpoch 19 | Train MSE: 0.089160 | Val MSE: 0.061631 | Test MSE: 0.081696\nEpoch 20 | Train MSE: 0.083793 | Val MSE: 0.071046 | Test MSE: 0.102161\nEpoch 21 | Train MSE: 0.098091 | Val MSE: 0.065141 | Test MSE: 0.086354\nEpoch 22 | Train MSE: 0.088768 | Val MSE: 0.070574 | Test MSE: 0.095810\nEpoch 23 | Train MSE: 0.082883 | Val MSE: 0.064559 | Test MSE: 0.084176\nEpoch 24 | Train MSE: 0.084618 | Val MSE: 0.066082 | Test MSE: 0.085252\nEpoch 25 | Train MSE: 0.078747 | Val MSE: 0.056492 | Test MSE: 0.086949\nEpoch 26 | Train MSE: 0.077451 | Val MSE: 0.066188 | Test MSE: 0.084701\nEpoch 27 | Train MSE: 0.081250 | Val MSE: 0.062980 | Test MSE: 0.092317\nEpoch 28 | Train MSE: 0.082330 | Val MSE: 0.061090 | Test MSE: 0.085896\nModels saved successfully.\nEpoch 29 | Train MSE: 0.080348 | Val MSE: 0.055502 | Test MSE: 0.081493\nModels saved successfully.\nEpoch 30 | Train MSE: 0.078899 | Val MSE: 0.054523 | Test MSE: 0.081883\nModels saved successfully.\nEpoch 31 | Train MSE: 0.073877 | Val MSE: 0.051020 | Test MSE: 0.069711\nEpoch 32 | Train MSE: 0.070997 | Val MSE: 0.069461 | Test MSE: 0.099804\nEpoch 33 | Train MSE: 0.076605 | Val MSE: 0.066530 | Test MSE: 0.086152\nEpoch 34 | Train MSE: 0.073484 | Val MSE: 0.050860 | Test MSE: 0.071723\nModels saved successfully.\nEpoch 35 | Train MSE: 0.064943 | Val MSE: 0.048091 | Test MSE: 0.063820\nEpoch 36 | Train MSE: 0.070856 | Val MSE: 0.051822 | Test MSE: 0.074372\nEpoch 37 | Train MSE: 0.068195 | Val MSE: 0.053001 | Test MSE: 0.076199\nModels saved successfully.\nEpoch 38 | Train MSE: 0.068502 | Val MSE: 0.047798 | Test MSE: 0.062167\nEpoch 39 | Train MSE: 0.067052 | Val MSE: 0.054344 | Test MSE: 0.070923\nEpoch 40 | Train MSE: 0.063167 | Val MSE: 0.065297 | Test MSE: 0.090870\nEpoch 41 | Train MSE: 0.074043 | Val MSE: 0.049826 | Test MSE: 0.068825\nEpoch 42 | Train MSE: 0.064296 | Val MSE: 0.049637 | Test MSE: 0.076130\nEpoch 43 | Train MSE: 0.068094 | Val MSE: 0.055826 | Test MSE: 0.072679\nEpoch 44 | Train MSE: 0.064619 | Val MSE: 0.075069 | Test MSE: 0.104861\nModels saved successfully.\nEpoch 45 | Train MSE: 0.067243 | Val MSE: 0.046677 | Test MSE: 0.062589\nEpoch 46 | Train MSE: 0.060830 | Val MSE: 0.046204 | Test MSE: 0.068818\nModels saved successfully.\nEpoch 47 | Train MSE: 0.062878 | Val MSE: 0.044749 | Test MSE: 0.062117\nEpoch 48 | Train MSE: 0.063607 | Val MSE: 0.050003 | Test MSE: 0.072662\nEpoch 49 | Train MSE: 0.064094 | Val MSE: 0.045894 | Test MSE: 0.063740\nEpoch 50 | Train MSE: 0.058910 | Val MSE: 0.052669 | Test MSE: 0.071942\nModels saved successfully.\nEpoch 51 | Train MSE: 0.059264 | Val MSE: 0.042825 | Test MSE: 0.063713\nEpoch 52 | Train MSE: 0.062917 | Val MSE: 0.046430 | Test MSE: 0.067099\nEpoch 53 | Train MSE: 0.057762 | Val MSE: 0.051864 | Test MSE: 0.070467\nEpoch 54 | Train MSE: 0.058937 | Val MSE: 0.046246 | Test MSE: 0.063731\nEpoch 55 | Train MSE: 0.060705 | Val MSE: 0.046177 | Test MSE: 0.064809\nModels saved successfully.\nEpoch 56 | Train MSE: 0.055693 | Val MSE: 0.044121 | Test MSE: 0.059164\nEpoch 57 | Train MSE: 0.066528 | Val MSE: 0.045317 | Test MSE: 0.071733\nModels saved successfully.\nEpoch 58 | Train MSE: 0.062173 | Val MSE: 0.044698 | Test MSE: 0.056958\nEpoch 59 | Train MSE: 0.055711 | Val MSE: 0.040408 | Test MSE: 0.062283\nEpoch 60 | Train MSE: 0.061327 | Val MSE: 0.050296 | Test MSE: 0.074042\nEpoch 61 | Train MSE: 0.054975 | Val MSE: 0.042418 | Test MSE: 0.059964\nEpoch 62 | Train MSE: 0.055187 | Val MSE: 0.048474 | Test MSE: 0.074161\nModels saved successfully.\nEpoch 63 | Train MSE: 0.057689 | Val MSE: 0.040408 | Test MSE: 0.059124\nEpoch 64 | Train MSE: 0.053889 | Val MSE: 0.043099 | Test MSE: 0.068413\nModels saved successfully.\nEpoch 65 | Train MSE: 0.064230 | Val MSE: 0.038658 | Test MSE: 0.059527\nEpoch 66 | Train MSE: 0.051473 | Val MSE: 0.045797 | Test MSE: 0.061779\nModels saved successfully.\nEpoch 67 | Train MSE: 0.052051 | Val MSE: 0.039696 | Test MSE: 0.058277\nEpoch 68 | Train MSE: 0.052852 | Val MSE: 0.041249 | Test MSE: 0.059225\nModels saved successfully.\nEpoch 69 | Train MSE: 0.049277 | Val MSE: 0.037658 | Test MSE: 0.058881\nEpoch 70 | Train MSE: 0.056802 | Val MSE: 0.044230 | Test MSE: 0.058071\nModels saved successfully.\nEpoch 71 | Train MSE: 0.051501 | Val MSE: 0.038153 | Test MSE: 0.054865\nEpoch 72 | Train MSE: 0.049100 | Val MSE: 0.042283 | Test MSE: 0.055293\nEpoch 73 | Train MSE: 0.056024 | Val MSE: 0.042140 | Test MSE: 0.060113\nEpoch 74 | Train MSE: 0.050309 | Val MSE: 0.036803 | Test MSE: 0.060529\nModels saved successfully.\nEpoch 75 | Train MSE: 0.047742 | Val MSE: 0.038232 | Test MSE: 0.054390\nModels saved successfully.\nEpoch 76 | Train MSE: 0.048261 | Val MSE: 0.036744 | Test MSE: 0.054879\nEpoch 77 | Train MSE: 0.051194 | Val MSE: 0.038665 | Test MSE: 0.060231\nEpoch 78 | Train MSE: 0.047169 | Val MSE: 0.036713 | Test MSE: 0.056040\nModels saved successfully.\nEpoch 79 | Train MSE: 0.048136 | Val MSE: 0.033530 | Test MSE: 0.050488\nEpoch 80 | Train MSE: 0.047550 | Val MSE: 0.039358 | Test MSE: 0.062977\nEpoch 81 | Train MSE: 0.052207 | Val MSE: 0.040148 | Test MSE: 0.055370\nEpoch 82 | Train MSE: 0.046450 | Val MSE: 0.041119 | Test MSE: 0.056596\nModels saved successfully.\nEpoch 83 | Train MSE: 0.053485 | Val MSE: 0.033240 | Test MSE: 0.048927\nEpoch 84 | Train MSE: 0.042894 | Val MSE: 0.034516 | Test MSE: 0.048207\nEpoch 85 | Train MSE: 0.050926 | Val MSE: 0.033354 | Test MSE: 0.052489\nEpoch 86 | Train MSE: 0.044685 | Val MSE: 0.033199 | Test MSE: 0.049580\nEpoch 87 | Train MSE: 0.047981 | Val MSE: 0.033508 | Test MSE: 0.050463\nModels saved successfully.\nEpoch 88 | Train MSE: 0.042383 | Val MSE: 0.032524 | Test MSE: 0.046067\nEpoch 89 | Train MSE: 0.047250 | Val MSE: 0.031854 | Test MSE: 0.048714\nEpoch 90 | Train MSE: 0.048333 | Val MSE: 0.033968 | Test MSE: 0.047586\nEpoch 91 | Train MSE: 0.045544 | Val MSE: 0.035781 | Test MSE: 0.053522\nEpoch 92 | Train MSE: 0.045895 | Val MSE: 0.033317 | Test MSE: 0.048207\nEpoch 93 | Train MSE: 0.042149 | Val MSE: 0.032836 | Test MSE: 0.050033\nEpoch 94 | Train MSE: 0.041670 | Val MSE: 0.034274 | Test MSE: 0.059374\nEpoch 95 | Train MSE: 0.055684 | Val MSE: 0.033878 | Test MSE: 0.050871\nEpoch 96 | Train MSE: 0.045631 | Val MSE: 0.032866 | Test MSE: 0.049798\nEpoch 97 | Train MSE: 0.041565 | Val MSE: 0.031896 | Test MSE: 0.047030\nEpoch 98 | Train MSE: 0.042763 | Val MSE: 0.032306 | Test MSE: 0.052455\nEpoch 99 | Train MSE: 0.043364 | Val MSE: 0.031489 | Test MSE: 0.054405\nEpoch 100 | Train MSE: 0.050499 | Val MSE: 0.032331 | Test MSE: 0.056086\n","output_type":"stream"}],"execution_count":14},{"id":"1f00e722-b355-4a83-b07d-e24363c8f55a","cell_type":"code","source":"for epoch in range(101, 151):\n    model.train()\n    train_loss = 0\n\n    for (x,) in train_loader:\n        x = x.to(device)\n        x_hat = model(x)\n        loss = loss_fn(x_hat, x)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n    val_loss = evaluate(model, val_loader)\n    test_loss = evaluate(model, test_loader)\n\n    if val_loss + test_loss < best_loss:\n        best_loss = val_loss + test_loss\n        # Saves the weights\n        torch.save(model.state_dict(), encoder_path)\n        print(\"Models saved successfully.\")\n\n    print(f\"Epoch {epoch:02d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f} | Test MSE: {test_loss:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:58:15.942323Z","iopub.execute_input":"2025-06-27T08:58:15.943074Z","iopub.status.idle":"2025-06-27T09:04:29.740202Z","shell.execute_reply.started":"2025-06-27T08:58:15.943048Z","shell.execute_reply":"2025-06-27T09:04:29.739319Z"}},"outputs":[{"name":"stdout","text":"Epoch 101 | Train MSE: 0.043482 | Val MSE: 0.033593 | Test MSE: 0.048350\nModels saved successfully.\nEpoch 102 | Train MSE: 0.044059 | Val MSE: 0.029884 | Test MSE: 0.047465\nEpoch 103 | Train MSE: 0.040157 | Val MSE: 0.030338 | Test MSE: 0.047257\nModels saved successfully.\nEpoch 104 | Train MSE: 0.040385 | Val MSE: 0.031780 | Test MSE: 0.044534\nEpoch 105 | Train MSE: 0.044626 | Val MSE: 0.030329 | Test MSE: 0.047372\nEpoch 106 | Train MSE: 0.041013 | Val MSE: 0.030838 | Test MSE: 0.052234\nEpoch 107 | Train MSE: 0.044169 | Val MSE: 0.029955 | Test MSE: 0.047066\nModels saved successfully.\nEpoch 108 | Train MSE: 0.039915 | Val MSE: 0.029572 | Test MSE: 0.042532\nEpoch 109 | Train MSE: 0.040812 | Val MSE: 0.028459 | Test MSE: 0.046656\nEpoch 110 | Train MSE: 0.046086 | Val MSE: 0.031689 | Test MSE: 0.049078\nEpoch 111 | Train MSE: 0.039199 | Val MSE: 0.033407 | Test MSE: 0.053452\nEpoch 112 | Train MSE: 0.039879 | Val MSE: 0.029803 | Test MSE: 0.044940\nEpoch 113 | Train MSE: 0.039956 | Val MSE: 0.032815 | Test MSE: 0.050572\nModels saved successfully.\nEpoch 114 | Train MSE: 0.047317 | Val MSE: 0.027776 | Test MSE: 0.041288\nEpoch 115 | Train MSE: 0.040970 | Val MSE: 0.028805 | Test MSE: 0.047939\nEpoch 116 | Train MSE: 0.040371 | Val MSE: 0.028929 | Test MSE: 0.046174\nEpoch 117 | Train MSE: 0.036454 | Val MSE: 0.028327 | Test MSE: 0.043748\nEpoch 118 | Train MSE: 0.038488 | Val MSE: 0.028381 | Test MSE: 0.046901\nEpoch 119 | Train MSE: 0.034782 | Val MSE: 0.027903 | Test MSE: 0.044697\nEpoch 120 | Train MSE: 0.038848 | Val MSE: 0.030288 | Test MSE: 0.051380\nEpoch 121 | Train MSE: 0.038548 | Val MSE: 0.031195 | Test MSE: 0.047477\nEpoch 122 | Train MSE: 0.036494 | Val MSE: 0.031691 | Test MSE: 0.051717\nEpoch 123 | Train MSE: 0.043996 | Val MSE: 0.030135 | Test MSE: 0.056132\nEpoch 124 | Train MSE: 0.038144 | Val MSE: 0.029142 | Test MSE: 0.049463\nEpoch 125 | Train MSE: 0.037790 | Val MSE: 0.028310 | Test MSE: 0.051679\nEpoch 126 | Train MSE: 0.033607 | Val MSE: 0.029673 | Test MSE: 0.054982\nEpoch 127 | Train MSE: 0.042205 | Val MSE: 0.029272 | Test MSE: 0.058065\nEpoch 128 | Train MSE: 0.040398 | Val MSE: 0.026704 | Test MSE: 0.044426\nEpoch 129 | Train MSE: 0.040293 | Val MSE: 0.029506 | Test MSE: 0.047967\nEpoch 130 | Train MSE: 0.042875 | Val MSE: 0.028783 | Test MSE: 0.042173\nEpoch 131 | Train MSE: 0.048226 | Val MSE: 0.036210 | Test MSE: 0.051186\nEpoch 132 | Train MSE: 0.041152 | Val MSE: 0.030840 | Test MSE: 0.050129\nEpoch 133 | Train MSE: 0.042140 | Val MSE: 0.028121 | Test MSE: 0.042254\nModels saved successfully.\nEpoch 134 | Train MSE: 0.035541 | Val MSE: 0.026827 | Test MSE: 0.040826\nEpoch 135 | Train MSE: 0.035056 | Val MSE: 0.040668 | Test MSE: 0.057817\nEpoch 136 | Train MSE: 0.039128 | Val MSE: 0.026479 | Test MSE: 0.044712\nEpoch 137 | Train MSE: 0.035609 | Val MSE: 0.028617 | Test MSE: 0.049025\nEpoch 138 | Train MSE: 0.037058 | Val MSE: 0.028902 | Test MSE: 0.057234\nEpoch 139 | Train MSE: 0.039913 | Val MSE: 0.027910 | Test MSE: 0.048513\nEpoch 140 | Train MSE: 0.033680 | Val MSE: 0.025684 | Test MSE: 0.051136\nEpoch 141 | Train MSE: 0.035528 | Val MSE: 0.026190 | Test MSE: 0.051155\nEpoch 142 | Train MSE: 0.035529 | Val MSE: 0.026648 | Test MSE: 0.044604\nEpoch 143 | Train MSE: 0.032359 | Val MSE: 0.026167 | Test MSE: 0.050232\nEpoch 144 | Train MSE: 0.038083 | Val MSE: 0.024757 | Test MSE: 0.044890\nModels saved successfully.\nEpoch 145 | Train MSE: 0.035664 | Val MSE: 0.024620 | Test MSE: 0.042926\nModels saved successfully.\nEpoch 146 | Train MSE: 0.033018 | Val MSE: 0.027621 | Test MSE: 0.039037\nEpoch 147 | Train MSE: 0.032613 | Val MSE: 0.033238 | Test MSE: 0.055774\nModels saved successfully.\nEpoch 148 | Train MSE: 0.038543 | Val MSE: 0.024598 | Test MSE: 0.040902\nEpoch 149 | Train MSE: 0.032280 | Val MSE: 0.025353 | Test MSE: 0.055079\nEpoch 150 | Train MSE: 0.038829 | Val MSE: 0.024426 | Test MSE: 0.044092\n","output_type":"stream"}],"execution_count":15},{"id":"5f3cc181-c113-4a1e-be3d-00b36128954b","cell_type":"code","source":"for epoch in range(151, 201):\n    model.train()\n    train_loss = 0\n\n    for (x,) in train_loader:\n        x = x.to(device)\n        x_hat = model(x)\n        loss = loss_fn(x_hat, x)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n    val_loss = evaluate(model, val_loader)\n    test_loss = evaluate(model, test_loader)\n\n    if val_loss + test_loss < best_loss:\n        best_loss = val_loss + test_loss\n        # Saves the weights\n        torch.save(model.state_dict(), encoder_path)\n        print(\"Models saved successfully.\")\n\n    print(f\"Epoch {epoch:02d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f} | Test MSE: {test_loss:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:05:07.664829Z","iopub.execute_input":"2025-06-27T09:05:07.665511Z","iopub.status.idle":"2025-06-27T09:11:19.813081Z","shell.execute_reply.started":"2025-06-27T09:05:07.665463Z","shell.execute_reply":"2025-06-27T09:11:19.812426Z"}},"outputs":[{"name":"stdout","text":"Epoch 151 | Train MSE: 0.035499 | Val MSE: 0.026993 | Test MSE: 0.047837\nEpoch 152 | Train MSE: 0.036469 | Val MSE: 0.024778 | Test MSE: 0.043585\nEpoch 153 | Train MSE: 0.037096 | Val MSE: 0.026355 | Test MSE: 0.048484\nEpoch 154 | Train MSE: 0.033922 | Val MSE: 0.025274 | Test MSE: 0.045618\nEpoch 155 | Train MSE: 0.031814 | Val MSE: 0.025095 | Test MSE: 0.050864\nEpoch 156 | Train MSE: 0.035852 | Val MSE: 0.025641 | Test MSE: 0.047051\nEpoch 157 | Train MSE: 0.035775 | Val MSE: 0.024176 | Test MSE: 0.044478\nEpoch 158 | Train MSE: 0.032847 | Val MSE: 0.024202 | Test MSE: 0.046343\nModels saved successfully.\nEpoch 159 | Train MSE: 0.037993 | Val MSE: 0.023369 | Test MSE: 0.040921\nModels saved successfully.\nEpoch 160 | Train MSE: 0.031047 | Val MSE: 0.023494 | Test MSE: 0.039360\nEpoch 161 | Train MSE: 0.034264 | Val MSE: 0.025722 | Test MSE: 0.040771\nEpoch 162 | Train MSE: 0.034376 | Val MSE: 0.024751 | Test MSE: 0.045739\nEpoch 163 | Train MSE: 0.029455 | Val MSE: 0.023503 | Test MSE: 0.049054\nEpoch 164 | Train MSE: 0.037707 | Val MSE: 0.025693 | Test MSE: 0.045800\nEpoch 165 | Train MSE: 0.031207 | Val MSE: 0.028104 | Test MSE: 0.042886\nEpoch 166 | Train MSE: 0.033988 | Val MSE: 0.024770 | Test MSE: 0.053761\nEpoch 167 | Train MSE: 0.032545 | Val MSE: 0.024983 | Test MSE: 0.039251\nEpoch 168 | Train MSE: 0.032139 | Val MSE: 0.023923 | Test MSE: 0.041026\nEpoch 169 | Train MSE: 0.038596 | Val MSE: 0.025472 | Test MSE: 0.044804\nEpoch 170 | Train MSE: 0.032819 | Val MSE: 0.025188 | Test MSE: 0.044148\nEpoch 171 | Train MSE: 0.034638 | Val MSE: 0.026157 | Test MSE: 0.040841\nEpoch 172 | Train MSE: 0.029507 | Val MSE: 0.025773 | Test MSE: 0.048426\nEpoch 173 | Train MSE: 0.033589 | Val MSE: 0.024767 | Test MSE: 0.051077\nModels saved successfully.\nEpoch 174 | Train MSE: 0.036460 | Val MSE: 0.023187 | Test MSE: 0.036628\nEpoch 175 | Train MSE: 0.028911 | Val MSE: 0.025153 | Test MSE: 0.045083\nEpoch 176 | Train MSE: 0.030752 | Val MSE: 0.024135 | Test MSE: 0.041626\nEpoch 177 | Train MSE: 0.035213 | Val MSE: 0.024360 | Test MSE: 0.045635\nEpoch 178 | Train MSE: 0.033211 | Val MSE: 0.027603 | Test MSE: 0.045990\nEpoch 179 | Train MSE: 0.031039 | Val MSE: 0.023367 | Test MSE: 0.042083\nEpoch 180 | Train MSE: 0.033089 | Val MSE: 0.023422 | Test MSE: 0.046118\nEpoch 181 | Train MSE: 0.033170 | Val MSE: 0.022650 | Test MSE: 0.042930\nEpoch 182 | Train MSE: 0.027776 | Val MSE: 0.026189 | Test MSE: 0.045425\nEpoch 183 | Train MSE: 0.031238 | Val MSE: 0.023128 | Test MSE: 0.041965\nEpoch 184 | Train MSE: 0.033618 | Val MSE: 0.022920 | Test MSE: 0.047399\nEpoch 185 | Train MSE: 0.030887 | Val MSE: 0.023928 | Test MSE: 0.040682\nEpoch 186 | Train MSE: 0.028540 | Val MSE: 0.021241 | Test MSE: 0.039051\nEpoch 187 | Train MSE: 0.031423 | Val MSE: 0.023171 | Test MSE: 0.042322\nEpoch 188 | Train MSE: 0.028233 | Val MSE: 0.026275 | Test MSE: 0.039786\nEpoch 189 | Train MSE: 0.029114 | Val MSE: 0.025041 | Test MSE: 0.041439\nEpoch 190 | Train MSE: 0.032459 | Val MSE: 0.022582 | Test MSE: 0.041471\nEpoch 191 | Train MSE: 0.030685 | Val MSE: 0.021475 | Test MSE: 0.042783\nEpoch 192 | Train MSE: 0.031310 | Val MSE: 0.023809 | Test MSE: 0.047212\nEpoch 193 | Train MSE: 0.031545 | Val MSE: 0.022760 | Test MSE: 0.047491\nEpoch 194 | Train MSE: 0.029458 | Val MSE: 0.029130 | Test MSE: 0.046390\nEpoch 195 | Train MSE: 0.031725 | Val MSE: 0.026704 | Test MSE: 0.082530\nEpoch 196 | Train MSE: 0.033104 | Val MSE: 0.022035 | Test MSE: 0.041228\nEpoch 197 | Train MSE: 0.028156 | Val MSE: 0.022436 | Test MSE: 0.047624\nEpoch 198 | Train MSE: 0.029561 | Val MSE: 0.022122 | Test MSE: 0.043844\nEpoch 199 | Train MSE: 0.027173 | Val MSE: 0.022696 | Test MSE: 0.041963\nEpoch 200 | Train MSE: 0.029322 | Val MSE: 0.022921 | Test MSE: 0.043799\n","output_type":"stream"}],"execution_count":16},{"id":"ad8499e3","cell_type":"markdown","source":"# Save the models","metadata":{}},{"id":"a31303e8","cell_type":"code","source":"\"\"\"encoder_path = \"/kaggle/working/encoder.pth\"\n\n# Saves the weights\ntorch.save(model.state_dict(), encoder_path)\n\nprint(\"Models saved successfully.\")\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:43:41.247265Z","iopub.status.idle":"2025-06-27T08:43:41.247499Z","shell.execute_reply.started":"2025-06-27T08:43:41.247378Z","shell.execute_reply":"2025-06-27T08:43:41.247388Z"}},"outputs":[],"execution_count":null},{"id":"446e901f","cell_type":"code","source":"model_loaded = TCNAutoencoder(input_dim=D, emb_dim=emb_dim, seq_len=T, channels=channels)\n\nmodel_loaded.load_state_dict(torch.load(encoder_path))\n\nmodel_loaded.to(device)\n\nmodel_loaded.eval()\n\nprint(\"Models reloaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:11:28.279343Z","iopub.execute_input":"2025-06-27T09:11:28.279652Z","iopub.status.idle":"2025-06-27T09:11:28.310657Z","shell.execute_reply.started":"2025-06-27T09:11:28.279629Z","shell.execute_reply":"2025-06-27T09:11:28.309891Z"}},"outputs":[{"name":"stdout","text":"Models reloaded successfully.\n","output_type":"stream"}],"execution_count":17}]}