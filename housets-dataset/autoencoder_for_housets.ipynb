{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc1d880",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be2f0f9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:16.337410Z",
     "iopub.status.busy": "2025-06-27T08:43:16.337219Z",
     "iopub.status.idle": "2025-06-27T08:43:24.560587Z",
     "shell.execute_reply": "2025-06-27T08:43:24.560003Z",
     "shell.execute_reply.started": "2025-06-27T08:43:16.337393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a740f43",
   "metadata": {},
   "source": [
    "# Data extraction and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a717c9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:24.561960Z",
     "iopub.status.busy": "2025-06-27T08:43:24.561660Z",
     "iopub.status.idle": "2025-06-27T08:43:24.996363Z",
     "shell.execute_reply": "2025-06-27T08:43:24.995808Z",
     "shell.execute_reply.started": "2025-06-27T08:43:24.561941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "csv_file = \"HouseTS_with_images.csv\"\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff7912ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:24.997322Z",
     "iopub.status.busy": "2025-06-27T08:43:24.997076Z",
     "iopub.status.idle": "2025-06-27T08:43:25.018089Z",
     "shell.execute_reply": "2025-06-27T08:43:25.017540Z",
     "shell.execute_reply.started": "2025-06-27T08:43:24.997300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of entries per zipcode: 142.00\n"
     ]
    }
   ],
   "source": [
    "# Count the number of entries per zipcode\n",
    "count_per_zipcode = df[\"zipcode\"].value_counts()\n",
    "\n",
    "# Calculate the average\n",
    "average_count = count_per_zipcode.mean()\n",
    "\n",
    "print(f\"Average number of entries per zipcode: {average_count:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d964f5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:25.018821Z",
     "iopub.status.busy": "2025-06-27T08:43:25.018659Z",
     "iopub.status.idle": "2025-06-27T08:43:25.070192Z",
     "shell.execute_reply": "2025-06-27T08:43:25.069680Z",
     "shell.execute_reply.started": "2025-06-27T08:43:25.018807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average timestep for zipcode 20001: 30 days 10:33:11.489361702\n",
      "In days: 30 days\n"
     ]
    }
   ],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Filter for the desired zipcode\n",
    "zipcode = 20001\n",
    "df_zip = df[df['zipcode'] == zipcode].sort_values('date')\n",
    "\n",
    "# Calculate the time differences between consecutive dates\n",
    "time_diffs = df_zip['date'].diff().dropna()\n",
    "\n",
    "# Calculate the average timestep (in days)\n",
    "average_timestep = time_diffs.mean()\n",
    "\n",
    "print(f\"Average timestep for zipcode {zipcode}: {average_timestep}\")\n",
    "print(f\"In days: {average_timestep.days} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888664ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:25.071947Z",
     "iopub.status.busy": "2025-06-27T08:43:25.071742Z",
     "iopub.status.idle": "2025-06-27T08:43:25.077199Z",
     "shell.execute_reply": "2025-06-27T08:43:25.076598Z",
     "shell.execute_reply.started": "2025-06-27T08:43:25.071932Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns in the dataset:\n",
      "date\n",
      "city\n",
      "city_full\n"
     ]
    }
   ],
   "source": [
    "# Select non-numeric columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "print(\"Non-numeric columns in the dataset:\")\n",
    "for col in non_numeric_cols:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08abc28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:25.078133Z",
     "iopub.status.busy": "2025-06-27T08:43:25.077897Z",
     "iopub.status.idle": "2025-06-27T08:43:25.103070Z",
     "shell.execute_reply": "2025-06-27T08:43:25.102349Z",
     "shell.execute_reply.started": "2025-06-27T08:43:25.078115Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the 'city' column:\n",
      "DC_Metro\n"
     ]
    }
   ],
   "source": [
    "unique_cities = df['city_full'].unique()\n",
    "\n",
    "print(\"Unique values in the 'city' column:\")\n",
    "for city in unique_cities:\n",
    "    print(city)\n",
    "\n",
    "df = df.drop(columns=['city', 'city_full'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc5d1845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:25.103956Z",
     "iopub.status.busy": "2025-06-27T08:43:25.103767Z",
     "iopub.status.idle": "2025-06-27T08:43:25.107563Z",
     "shell.execute_reply": "2025-06-27T08:43:25.106825Z",
     "shell.execute_reply.started": "2025-06-27T08:43:25.103941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "window_length = 12\n",
    "step_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3db128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:25.108579Z",
     "iopub.status.busy": "2025-06-27T08:43:25.108328Z",
     "iopub.status.idle": "2025-06-27T08:43:36.353386Z",
     "shell.execute_reply": "2025-06-27T08:43:36.352561Z",
     "shell.execute_reply.started": "2025-06-27T08:43:25.108562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (40348, 12, 34)\n",
      "Number of metadata entries: 40348\n",
      "Example metadata for window 0: (20001, Timestamp('2012-03-31 00:00:00'), 2012)\n"
     ]
    }
   ],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['zipcode', 'date']).reset_index(drop=True)\n",
    "\n",
    "all_windows = []\n",
    "all_meta = []\n",
    "\n",
    "def extract_windows(df_sub, window_len, step):\n",
    "    windows = []\n",
    "    for i in range(0, len(df_sub) - window_len + 1, step):\n",
    "        window = df_sub.iloc[i:i+window_len].drop(columns=['date', 'zipcode', 'year']).values\n",
    "        windows.append(window)\n",
    "    return np.array(windows)\n",
    "\n",
    "# Extract windows for each zipcode\n",
    "for zipcode, group in df.groupby('zipcode'):\n",
    "    group = group.reset_index(drop=True)\n",
    "    if len(group) < window_length:\n",
    "        continue\n",
    "\n",
    "    windows = extract_windows(group, window_length, step_size)\n",
    "    all_windows.append(windows)\n",
    "\n",
    "    # Save metadata: for example, the start date and zipcode of the window\n",
    "    window_meta = [(zipcode, group.loc[i, 'date'], group.loc[i, 'year']) for i in range(len(group) - window_length + 1)]\n",
    "    all_meta.extend(window_meta)\n",
    "\n",
    "# Concatenate all windows\n",
    "X = np.vstack(all_windows)  # shape: (num_samples, window_length, num_features without date/zipcode)\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Number of metadata entries:\", len(all_meta))\n",
    "\n",
    "# Example of printing first metadata\n",
    "print(\"Example metadata for window 0:\", all_meta[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e3e4b15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:36.354607Z",
     "iopub.status.busy": "2025-06-27T08:43:36.354288Z",
     "iopub.status.idle": "2025-06-27T08:43:36.802316Z",
     "shell.execute_reply": "2025-06-27T08:43:36.801534Z",
     "shell.execute_reply.started": "2025-06-27T08:43:36.354580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, window_ids = shuffle(X, all_meta, random_state=42)\n",
    "\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.1 * len(X))\n",
    "test_size = len(X) - train_size - val_size\n",
    "\n",
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:train_size + val_size]\n",
    "X_test = X[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661cfd8b",
   "metadata": {},
   "source": [
    "# Encoder and Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359a70b",
   "metadata": {},
   "source": [
    "Our goal was to build an encoder-decoder model able to learn a compressed representaion of the input time series, so to allow a more efficient search of similar time series in a smaller dimensional space, speeding up the task of finding k nearest neighbours. </br> The encoder gets as input a tensor of shape (batch_size, seq_len, num_features) and compresses it into a tensor of shape (batch_size, embedding_dim), while the decoder takes the output of the encoder and tries to reconstruct the original input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a9a7560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:36.803692Z",
     "iopub.status.busy": "2025-06-27T08:43:36.803311Z",
     "iopub.status.idle": "2025-06-27T08:43:36.820937Z",
     "shell.execute_reply": "2025-06-27T08:43:36.820372Z",
     "shell.execute_reply.started": "2025-06-27T08:43:36.803670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size] if self.chomp_size > 0 else x\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) * dilation  # full causal\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                               padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size,\n",
    "                               padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "            self.conv2, self.chomp2, self.relu2, self.dropout2,\n",
    "        )\n",
    "\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) \\\n",
    "            if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        out = self.net(x)\n",
    "        if out.shape != res.shape:\n",
    "            # Align time dimension by cropping the residual (this might be necessary in some edge cases)\n",
    "            min_len = min(out.size(-1), res.size(-1))\n",
    "            out = out[..., :min_len]\n",
    "            res = res[..., :min_len]\n",
    "        return self.relu(out + res)\n",
    "\n",
    "# Encoder\n",
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(num_channels)):\n",
    "            in_ch = input_dim if i == 0 else num_channels[i - 1]\n",
    "            out_ch = num_channels[i]\n",
    "            dilation = 2 ** i\n",
    "            layers.append(TemporalBlock(in_ch, out_ch, kernel_size, dilation, dropout))\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "\n",
    "        # Projection from [B, C, T] to [B, emb_dim]\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.project = nn.Linear(num_channels[-1], emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, D] → [B, D, T]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.tcn(x)  # [B, C, T]\n",
    "        x = self.pool(x).squeeze(-1)  # [B, C]\n",
    "        x = self.project(x)  # [B, emb_dim]\n",
    "        return x\n",
    "\n",
    "# Decoder\n",
    "class TCNDecoder(nn.Module):\n",
    "    def __init__(self, emb_dim, output_dim, seq_len, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Project embedding back to a sequence shape: [B, C, T]\n",
    "        self.expand = nn.Linear(emb_dim, num_channels[0] * seq_len)\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(num_channels) - 1):\n",
    "            in_ch = num_channels[i]\n",
    "            out_ch = num_channels[i + 1]\n",
    "            dilation = 2 ** i\n",
    "            layers.append(TemporalBlock(in_ch, out_ch, kernel_size, dilation, dropout))\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.out_proj = nn.Conv1d(num_channels[-1], output_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, emb_dim] → [B, C0, T]\n",
    "        x = self.expand(x)  # [B, C0 * T]\n",
    "        x = x.view(x.size(0), -1, self.seq_len)  # [B, C0, T]\n",
    "        x = self.tcn(x)  # [B, Cn, T]\n",
    "        x = self.out_proj(x)  # [B, D, T]\n",
    "        return x.permute(0, 2, 1)  # [B, T, D]\n",
    "    \n",
    "class TCNAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, seq_len, channels, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = TCNEncoder(input_dim, emb_dim, channels, kernel_size, dropout)\n",
    "        self.decoder = TCNDecoder(emb_dim, input_dim, seq_len, channels[::-1], kernel_size, dropout)\n",
    "\n",
    "    def forward(self, x, only_encoder = False):\n",
    "        # x: [B, T, D]\n",
    "        z = self.encoder(x)       # [B, emb_dim]\n",
    "        if only_encoder:\n",
    "            return z\n",
    "        x_recon = self.decoder(z) # [B, T, D]\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d42669af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:36.821897Z",
     "iopub.status.busy": "2025-06-27T08:43:36.821647Z",
     "iopub.status.idle": "2025-06-27T08:43:37.109376Z",
     "shell.execute_reply": "2025-06-27T08:43:37.108733Z",
     "shell.execute_reply.started": "2025-06-27T08:43:36.821871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4.54852826e+05 5.58716962e+05 2.27982484e+02 2.46098550e+02\n",
      "   6.83864232e+01 7.45339848e+01 8.01239924e+01 4.91713935e+01\n",
      "   5.41263056e+01 9.88508572e-01 2.73734409e-01 3.22142625e-01\n",
      "   2.36917260e+01 7.18886096e-01 2.33493255e+00 1.18643558e+00\n",
      "   8.91169936e+01 9.04543250e+01 5.97777915e+01 4.15741364e+00\n",
      "   1.16968157e+01 1.90285746e+04 3.99743190e+01 4.78702971e+04\n",
      "   1.87499750e+04 7.36403461e+03 1.59877327e+03 4.40833370e+05\n",
      "   1.08915696e+04 6.19120623e+02 1.82830710e+04 1.82830710e+04\n",
      "   9.33441450e+03 4.57644868e+05]]] [[[2.47671163e+05 8.77060063e+06 1.15036342e+02 5.82713229e+02\n",
      "   7.01868096e+01 7.54677556e+01 8.32473874e+01 4.95942947e+01\n",
      "   7.83625340e+01 3.22186448e-02 1.94536395e-01 2.48884578e-01\n",
      "   4.77475403e+01 1.64222823e+00 4.26626174e+00 1.79608954e+00\n",
      "   1.57134652e+02 1.90464107e+02 8.16188975e+01 8.63331047e+00\n",
      "   1.59101982e+01 1.75557344e+04 7.11650516e+00 1.92278578e+04\n",
      "   1.73431754e+04 6.86866352e+03 4.32730094e+02 1.83839663e+05\n",
      "   1.01493864e+04 7.54798540e+02 1.68373952e+04 1.68373952e+04\n",
      "   8.72958115e+03 2.01021908e+05]]]\n",
      "[[[ 1.04901885e-15 -1.83335446e-16 -6.88877990e-14 -1.39840261e-14\n",
      "   -6.03017553e-16 -1.14152231e-14 -6.21827718e-16 -6.56158331e-16\n",
      "   -2.27563683e-17  2.87788068e-13  6.17798562e-13  2.99696542e-13\n",
      "    3.46190048e-15 -2.62387543e-14 -5.26354523e-15 -6.06668111e-15\n",
      "   -1.31158943e-15  6.39401535e-15  6.03822746e-17  7.73458912e-15\n",
      "   -1.10211213e-16  7.42377132e-16 -7.47775551e-13 -7.42611270e-17\n",
      "    7.41233794e-17  6.64851009e-17  3.82536899e-16  7.23618967e-17\n",
      "   -1.50980623e-16  5.57425705e-16 -1.70948585e-16 -1.70948585e-16\n",
      "   -3.01865348e-16 -4.81838434e-15]]] [[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "   1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the statistics of the train dataset and normalize with respect to it \n",
    "\n",
    "mean = X_train.mean(axis=(0, 1), keepdims=True)  # shape (1, 1, num_features)\n",
    "std = X_train.std(axis=(0, 1), keepdims=True)\n",
    "\n",
    "X_train_norm = (X_train - mean) / std\n",
    "X_val_norm = (X_val - mean) / std\n",
    "X_test_norm = (X_test - mean) / std\n",
    "\n",
    "print(mean, std)\n",
    "\n",
    "mean = X_train_norm.mean(axis=(0, 1), keepdims=True)  # shape (1, 1, num_features)\n",
    "std = X_train_norm.std(axis=(0, 1), keepdims=True)\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bdd7eea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:43:37.110384Z",
     "iopub.status.busy": "2025-06-27T08:43:37.110152Z",
     "iopub.status.idle": "2025-06-27T08:43:37.210507Z",
     "shell.execute_reply": "2025-06-27T08:43:37.209939Z",
     "shell.execute_reply.started": "2025-06-27T08:43:37.110366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_norm, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_norm, dtype=torch.float32)\n",
    "\n",
    "# Dataset & DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec739f61",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac80033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:44:38.254987Z",
     "iopub.status.busy": "2025-06-27T08:44:38.254702Z",
     "iopub.status.idle": "2025-06-27T08:57:05.875341Z",
     "shell.execute_reply": "2025-06-27T08:57:05.874701Z",
     "shell.execute_reply.started": "2025-06-27T08:44:38.254968Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 276,322\n",
      "Models saved successfully.\n",
      "Epoch 01 | Train MSE: 0.329717 | Val MSE: 0.194305 | Test MSE: 0.226819\n",
      "Models saved successfully.\n",
      "Epoch 02 | Train MSE: 0.211920 | Val MSE: 0.149754 | Test MSE: 0.189440\n",
      "Models saved successfully.\n",
      "Epoch 03 | Train MSE: 0.182255 | Val MSE: 0.138717 | Test MSE: 0.172429\n",
      "Models saved successfully.\n",
      "Epoch 04 | Train MSE: 0.159782 | Val MSE: 0.105315 | Test MSE: 0.146662\n",
      "Epoch 05 | Train MSE: 0.141739 | Val MSE: 0.111816 | Test MSE: 0.143787\n",
      "Models saved successfully.\n",
      "Epoch 06 | Train MSE: 0.141896 | Val MSE: 0.099963 | Test MSE: 0.134545\n",
      "Models saved successfully.\n",
      "Epoch 07 | Train MSE: 0.132991 | Val MSE: 0.098059 | Test MSE: 0.122779\n",
      "Models saved successfully.\n",
      "Epoch 08 | Train MSE: 0.124939 | Val MSE: 0.088298 | Test MSE: 0.110709\n",
      "Models saved successfully.\n",
      "Epoch 09 | Train MSE: 0.125151 | Val MSE: 0.085099 | Test MSE: 0.110870\n",
      "Models saved successfully.\n",
      "Epoch 10 | Train MSE: 0.115700 | Val MSE: 0.081556 | Test MSE: 0.105681\n",
      "Epoch 11 | Train MSE: 0.109634 | Val MSE: 0.081358 | Test MSE: 0.113280\n",
      "Models saved successfully.\n",
      "Epoch 12 | Train MSE: 0.108042 | Val MSE: 0.074726 | Test MSE: 0.095963\n",
      "Epoch 13 | Train MSE: 0.099981 | Val MSE: 0.075216 | Test MSE: 0.108042\n",
      "Epoch 14 | Train MSE: 0.101760 | Val MSE: 0.078464 | Test MSE: 0.096210\n",
      "Epoch 15 | Train MSE: 0.098398 | Val MSE: 0.078726 | Test MSE: 0.093063\n",
      "Models saved successfully.\n",
      "Epoch 16 | Train MSE: 0.094490 | Val MSE: 0.065578 | Test MSE: 0.091645\n",
      "Epoch 17 | Train MSE: 0.091457 | Val MSE: 0.076650 | Test MSE: 0.091379\n",
      "Epoch 18 | Train MSE: 0.094312 | Val MSE: 0.070610 | Test MSE: 0.097959\n",
      "Models saved successfully.\n",
      "Epoch 19 | Train MSE: 0.089160 | Val MSE: 0.061631 | Test MSE: 0.081696\n",
      "Epoch 20 | Train MSE: 0.083793 | Val MSE: 0.071046 | Test MSE: 0.102161\n",
      "Epoch 21 | Train MSE: 0.098091 | Val MSE: 0.065141 | Test MSE: 0.086354\n",
      "Epoch 22 | Train MSE: 0.088768 | Val MSE: 0.070574 | Test MSE: 0.095810\n",
      "Epoch 23 | Train MSE: 0.082883 | Val MSE: 0.064559 | Test MSE: 0.084176\n",
      "Epoch 24 | Train MSE: 0.084618 | Val MSE: 0.066082 | Test MSE: 0.085252\n",
      "Epoch 25 | Train MSE: 0.078747 | Val MSE: 0.056492 | Test MSE: 0.086949\n",
      "Epoch 26 | Train MSE: 0.077451 | Val MSE: 0.066188 | Test MSE: 0.084701\n",
      "Epoch 27 | Train MSE: 0.081250 | Val MSE: 0.062980 | Test MSE: 0.092317\n",
      "Epoch 28 | Train MSE: 0.082330 | Val MSE: 0.061090 | Test MSE: 0.085896\n",
      "Models saved successfully.\n",
      "Epoch 29 | Train MSE: 0.080348 | Val MSE: 0.055502 | Test MSE: 0.081493\n",
      "Models saved successfully.\n",
      "Epoch 30 | Train MSE: 0.078899 | Val MSE: 0.054523 | Test MSE: 0.081883\n",
      "Models saved successfully.\n",
      "Epoch 31 | Train MSE: 0.073877 | Val MSE: 0.051020 | Test MSE: 0.069711\n",
      "Epoch 32 | Train MSE: 0.070997 | Val MSE: 0.069461 | Test MSE: 0.099804\n",
      "Epoch 33 | Train MSE: 0.076605 | Val MSE: 0.066530 | Test MSE: 0.086152\n",
      "Epoch 34 | Train MSE: 0.073484 | Val MSE: 0.050860 | Test MSE: 0.071723\n",
      "Models saved successfully.\n",
      "Epoch 35 | Train MSE: 0.064943 | Val MSE: 0.048091 | Test MSE: 0.063820\n",
      "Epoch 36 | Train MSE: 0.070856 | Val MSE: 0.051822 | Test MSE: 0.074372\n",
      "Epoch 37 | Train MSE: 0.068195 | Val MSE: 0.053001 | Test MSE: 0.076199\n",
      "Models saved successfully.\n",
      "Epoch 38 | Train MSE: 0.068502 | Val MSE: 0.047798 | Test MSE: 0.062167\n",
      "Epoch 39 | Train MSE: 0.067052 | Val MSE: 0.054344 | Test MSE: 0.070923\n",
      "Epoch 40 | Train MSE: 0.063167 | Val MSE: 0.065297 | Test MSE: 0.090870\n",
      "Epoch 41 | Train MSE: 0.074043 | Val MSE: 0.049826 | Test MSE: 0.068825\n",
      "Epoch 42 | Train MSE: 0.064296 | Val MSE: 0.049637 | Test MSE: 0.076130\n",
      "Epoch 43 | Train MSE: 0.068094 | Val MSE: 0.055826 | Test MSE: 0.072679\n",
      "Epoch 44 | Train MSE: 0.064619 | Val MSE: 0.075069 | Test MSE: 0.104861\n",
      "Models saved successfully.\n",
      "Epoch 45 | Train MSE: 0.067243 | Val MSE: 0.046677 | Test MSE: 0.062589\n",
      "Epoch 46 | Train MSE: 0.060830 | Val MSE: 0.046204 | Test MSE: 0.068818\n",
      "Models saved successfully.\n",
      "Epoch 47 | Train MSE: 0.062878 | Val MSE: 0.044749 | Test MSE: 0.062117\n",
      "Epoch 48 | Train MSE: 0.063607 | Val MSE: 0.050003 | Test MSE: 0.072662\n",
      "Epoch 49 | Train MSE: 0.064094 | Val MSE: 0.045894 | Test MSE: 0.063740\n",
      "Epoch 50 | Train MSE: 0.058910 | Val MSE: 0.052669 | Test MSE: 0.071942\n",
      "Models saved successfully.\n",
      "Epoch 51 | Train MSE: 0.059264 | Val MSE: 0.042825 | Test MSE: 0.063713\n",
      "Epoch 52 | Train MSE: 0.062917 | Val MSE: 0.046430 | Test MSE: 0.067099\n",
      "Epoch 53 | Train MSE: 0.057762 | Val MSE: 0.051864 | Test MSE: 0.070467\n",
      "Epoch 54 | Train MSE: 0.058937 | Val MSE: 0.046246 | Test MSE: 0.063731\n",
      "Epoch 55 | Train MSE: 0.060705 | Val MSE: 0.046177 | Test MSE: 0.064809\n",
      "Models saved successfully.\n",
      "Epoch 56 | Train MSE: 0.055693 | Val MSE: 0.044121 | Test MSE: 0.059164\n",
      "Epoch 57 | Train MSE: 0.066528 | Val MSE: 0.045317 | Test MSE: 0.071733\n",
      "Models saved successfully.\n",
      "Epoch 58 | Train MSE: 0.062173 | Val MSE: 0.044698 | Test MSE: 0.056958\n",
      "Epoch 59 | Train MSE: 0.055711 | Val MSE: 0.040408 | Test MSE: 0.062283\n",
      "Epoch 60 | Train MSE: 0.061327 | Val MSE: 0.050296 | Test MSE: 0.074042\n",
      "Epoch 61 | Train MSE: 0.054975 | Val MSE: 0.042418 | Test MSE: 0.059964\n",
      "Epoch 62 | Train MSE: 0.055187 | Val MSE: 0.048474 | Test MSE: 0.074161\n",
      "Models saved successfully.\n",
      "Epoch 63 | Train MSE: 0.057689 | Val MSE: 0.040408 | Test MSE: 0.059124\n",
      "Epoch 64 | Train MSE: 0.053889 | Val MSE: 0.043099 | Test MSE: 0.068413\n",
      "Models saved successfully.\n",
      "Epoch 65 | Train MSE: 0.064230 | Val MSE: 0.038658 | Test MSE: 0.059527\n",
      "Epoch 66 | Train MSE: 0.051473 | Val MSE: 0.045797 | Test MSE: 0.061779\n",
      "Models saved successfully.\n",
      "Epoch 67 | Train MSE: 0.052051 | Val MSE: 0.039696 | Test MSE: 0.058277\n",
      "Epoch 68 | Train MSE: 0.052852 | Val MSE: 0.041249 | Test MSE: 0.059225\n",
      "Models saved successfully.\n",
      "Epoch 69 | Train MSE: 0.049277 | Val MSE: 0.037658 | Test MSE: 0.058881\n",
      "Epoch 70 | Train MSE: 0.056802 | Val MSE: 0.044230 | Test MSE: 0.058071\n",
      "Models saved successfully.\n",
      "Epoch 71 | Train MSE: 0.051501 | Val MSE: 0.038153 | Test MSE: 0.054865\n",
      "Epoch 72 | Train MSE: 0.049100 | Val MSE: 0.042283 | Test MSE: 0.055293\n",
      "Epoch 73 | Train MSE: 0.056024 | Val MSE: 0.042140 | Test MSE: 0.060113\n",
      "Epoch 74 | Train MSE: 0.050309 | Val MSE: 0.036803 | Test MSE: 0.060529\n",
      "Models saved successfully.\n",
      "Epoch 75 | Train MSE: 0.047742 | Val MSE: 0.038232 | Test MSE: 0.054390\n",
      "Models saved successfully.\n",
      "Epoch 76 | Train MSE: 0.048261 | Val MSE: 0.036744 | Test MSE: 0.054879\n",
      "Epoch 77 | Train MSE: 0.051194 | Val MSE: 0.038665 | Test MSE: 0.060231\n",
      "Epoch 78 | Train MSE: 0.047169 | Val MSE: 0.036713 | Test MSE: 0.056040\n",
      "Models saved successfully.\n",
      "Epoch 79 | Train MSE: 0.048136 | Val MSE: 0.033530 | Test MSE: 0.050488\n",
      "Epoch 80 | Train MSE: 0.047550 | Val MSE: 0.039358 | Test MSE: 0.062977\n",
      "Epoch 81 | Train MSE: 0.052207 | Val MSE: 0.040148 | Test MSE: 0.055370\n",
      "Epoch 82 | Train MSE: 0.046450 | Val MSE: 0.041119 | Test MSE: 0.056596\n",
      "Models saved successfully.\n",
      "Epoch 83 | Train MSE: 0.053485 | Val MSE: 0.033240 | Test MSE: 0.048927\n",
      "Epoch 84 | Train MSE: 0.042894 | Val MSE: 0.034516 | Test MSE: 0.048207\n",
      "Epoch 85 | Train MSE: 0.050926 | Val MSE: 0.033354 | Test MSE: 0.052489\n",
      "Epoch 86 | Train MSE: 0.044685 | Val MSE: 0.033199 | Test MSE: 0.049580\n",
      "Epoch 87 | Train MSE: 0.047981 | Val MSE: 0.033508 | Test MSE: 0.050463\n",
      "Models saved successfully.\n",
      "Epoch 88 | Train MSE: 0.042383 | Val MSE: 0.032524 | Test MSE: 0.046067\n",
      "Epoch 89 | Train MSE: 0.047250 | Val MSE: 0.031854 | Test MSE: 0.048714\n",
      "Epoch 90 | Train MSE: 0.048333 | Val MSE: 0.033968 | Test MSE: 0.047586\n",
      "Epoch 91 | Train MSE: 0.045544 | Val MSE: 0.035781 | Test MSE: 0.053522\n",
      "Epoch 92 | Train MSE: 0.045895 | Val MSE: 0.033317 | Test MSE: 0.048207\n",
      "Epoch 93 | Train MSE: 0.042149 | Val MSE: 0.032836 | Test MSE: 0.050033\n",
      "Epoch 94 | Train MSE: 0.041670 | Val MSE: 0.034274 | Test MSE: 0.059374\n",
      "Epoch 95 | Train MSE: 0.055684 | Val MSE: 0.033878 | Test MSE: 0.050871\n",
      "Epoch 96 | Train MSE: 0.045631 | Val MSE: 0.032866 | Test MSE: 0.049798\n",
      "Epoch 97 | Train MSE: 0.041565 | Val MSE: 0.031896 | Test MSE: 0.047030\n",
      "Epoch 98 | Train MSE: 0.042763 | Val MSE: 0.032306 | Test MSE: 0.052455\n",
      "Epoch 99 | Train MSE: 0.043364 | Val MSE: 0.031489 | Test MSE: 0.054405\n",
      "Epoch 100 | Train MSE: 0.050499 | Val MSE: 0.032331 | Test MSE: 0.056086\n"
     ]
    }
   ],
   "source": [
    "seq_len = X_train.shape[1]\n",
    "num_features = X_train.shape[2]\n",
    "\n",
    "# Parameters\n",
    "B, T, D = batch_size, seq_len, num_features\n",
    "emb_dim = 64\n",
    "channels = [32, 64, 128]\n",
    "\n",
    "# Instantiate model\n",
    "model = TCNAutoencoder(input_dim=D, emb_dim=emb_dim, seq_len=T, channels=channels).to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "params = list(model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "best_loss = 1e10\n",
    "\n",
    "encoder_path = \"/kaggle/working/encoder.pth\"\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (x,) in loader:\n",
    "            x = x.to(device)\n",
    "            x_hat = model(x)\n",
    "            loss = loss_fn(x_hat, x)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for (x,) in train_loader:\n",
    "        x = x.to(device)\n",
    "        x_hat = model(x)\n",
    "        loss = loss_fn(x_hat, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    test_loss = evaluate(model, test_loader)\n",
    "\n",
    "    if val_loss + test_loss < best_loss:\n",
    "        best_loss = val_loss + test_loss\n",
    "        # Saves the weights\n",
    "        torch.save(model.state_dict(), encoder_path)\n",
    "        print(\"Models saved successfully.\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f} | Test MSE: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f00e722-b355-4a83-b07d-e24363c8f55a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T08:58:15.943074Z",
     "iopub.status.busy": "2025-06-27T08:58:15.942323Z",
     "iopub.status.idle": "2025-06-27T09:04:29.740202Z",
     "shell.execute_reply": "2025-06-27T09:04:29.739319Z",
     "shell.execute_reply.started": "2025-06-27T08:58:15.943048Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101 | Train MSE: 0.043482 | Val MSE: 0.033593 | Test MSE: 0.048350\n",
      "Models saved successfully.\n",
      "Epoch 102 | Train MSE: 0.044059 | Val MSE: 0.029884 | Test MSE: 0.047465\n",
      "Epoch 103 | Train MSE: 0.040157 | Val MSE: 0.030338 | Test MSE: 0.047257\n",
      "Models saved successfully.\n",
      "Epoch 104 | Train MSE: 0.040385 | Val MSE: 0.031780 | Test MSE: 0.044534\n",
      "Epoch 105 | Train MSE: 0.044626 | Val MSE: 0.030329 | Test MSE: 0.047372\n",
      "Epoch 106 | Train MSE: 0.041013 | Val MSE: 0.030838 | Test MSE: 0.052234\n",
      "Epoch 107 | Train MSE: 0.044169 | Val MSE: 0.029955 | Test MSE: 0.047066\n",
      "Models saved successfully.\n",
      "Epoch 108 | Train MSE: 0.039915 | Val MSE: 0.029572 | Test MSE: 0.042532\n",
      "Epoch 109 | Train MSE: 0.040812 | Val MSE: 0.028459 | Test MSE: 0.046656\n",
      "Epoch 110 | Train MSE: 0.046086 | Val MSE: 0.031689 | Test MSE: 0.049078\n",
      "Epoch 111 | Train MSE: 0.039199 | Val MSE: 0.033407 | Test MSE: 0.053452\n",
      "Epoch 112 | Train MSE: 0.039879 | Val MSE: 0.029803 | Test MSE: 0.044940\n",
      "Epoch 113 | Train MSE: 0.039956 | Val MSE: 0.032815 | Test MSE: 0.050572\n",
      "Models saved successfully.\n",
      "Epoch 114 | Train MSE: 0.047317 | Val MSE: 0.027776 | Test MSE: 0.041288\n",
      "Epoch 115 | Train MSE: 0.040970 | Val MSE: 0.028805 | Test MSE: 0.047939\n",
      "Epoch 116 | Train MSE: 0.040371 | Val MSE: 0.028929 | Test MSE: 0.046174\n",
      "Epoch 117 | Train MSE: 0.036454 | Val MSE: 0.028327 | Test MSE: 0.043748\n",
      "Epoch 118 | Train MSE: 0.038488 | Val MSE: 0.028381 | Test MSE: 0.046901\n",
      "Epoch 119 | Train MSE: 0.034782 | Val MSE: 0.027903 | Test MSE: 0.044697\n",
      "Epoch 120 | Train MSE: 0.038848 | Val MSE: 0.030288 | Test MSE: 0.051380\n",
      "Epoch 121 | Train MSE: 0.038548 | Val MSE: 0.031195 | Test MSE: 0.047477\n",
      "Epoch 122 | Train MSE: 0.036494 | Val MSE: 0.031691 | Test MSE: 0.051717\n",
      "Epoch 123 | Train MSE: 0.043996 | Val MSE: 0.030135 | Test MSE: 0.056132\n",
      "Epoch 124 | Train MSE: 0.038144 | Val MSE: 0.029142 | Test MSE: 0.049463\n",
      "Epoch 125 | Train MSE: 0.037790 | Val MSE: 0.028310 | Test MSE: 0.051679\n",
      "Epoch 126 | Train MSE: 0.033607 | Val MSE: 0.029673 | Test MSE: 0.054982\n",
      "Epoch 127 | Train MSE: 0.042205 | Val MSE: 0.029272 | Test MSE: 0.058065\n",
      "Epoch 128 | Train MSE: 0.040398 | Val MSE: 0.026704 | Test MSE: 0.044426\n",
      "Epoch 129 | Train MSE: 0.040293 | Val MSE: 0.029506 | Test MSE: 0.047967\n",
      "Epoch 130 | Train MSE: 0.042875 | Val MSE: 0.028783 | Test MSE: 0.042173\n",
      "Epoch 131 | Train MSE: 0.048226 | Val MSE: 0.036210 | Test MSE: 0.051186\n",
      "Epoch 132 | Train MSE: 0.041152 | Val MSE: 0.030840 | Test MSE: 0.050129\n",
      "Epoch 133 | Train MSE: 0.042140 | Val MSE: 0.028121 | Test MSE: 0.042254\n",
      "Models saved successfully.\n",
      "Epoch 134 | Train MSE: 0.035541 | Val MSE: 0.026827 | Test MSE: 0.040826\n",
      "Epoch 135 | Train MSE: 0.035056 | Val MSE: 0.040668 | Test MSE: 0.057817\n",
      "Epoch 136 | Train MSE: 0.039128 | Val MSE: 0.026479 | Test MSE: 0.044712\n",
      "Epoch 137 | Train MSE: 0.035609 | Val MSE: 0.028617 | Test MSE: 0.049025\n",
      "Epoch 138 | Train MSE: 0.037058 | Val MSE: 0.028902 | Test MSE: 0.057234\n",
      "Epoch 139 | Train MSE: 0.039913 | Val MSE: 0.027910 | Test MSE: 0.048513\n",
      "Epoch 140 | Train MSE: 0.033680 | Val MSE: 0.025684 | Test MSE: 0.051136\n",
      "Epoch 141 | Train MSE: 0.035528 | Val MSE: 0.026190 | Test MSE: 0.051155\n",
      "Epoch 142 | Train MSE: 0.035529 | Val MSE: 0.026648 | Test MSE: 0.044604\n",
      "Epoch 143 | Train MSE: 0.032359 | Val MSE: 0.026167 | Test MSE: 0.050232\n",
      "Epoch 144 | Train MSE: 0.038083 | Val MSE: 0.024757 | Test MSE: 0.044890\n",
      "Models saved successfully.\n",
      "Epoch 145 | Train MSE: 0.035664 | Val MSE: 0.024620 | Test MSE: 0.042926\n",
      "Models saved successfully.\n",
      "Epoch 146 | Train MSE: 0.033018 | Val MSE: 0.027621 | Test MSE: 0.039037\n",
      "Epoch 147 | Train MSE: 0.032613 | Val MSE: 0.033238 | Test MSE: 0.055774\n",
      "Models saved successfully.\n",
      "Epoch 148 | Train MSE: 0.038543 | Val MSE: 0.024598 | Test MSE: 0.040902\n",
      "Epoch 149 | Train MSE: 0.032280 | Val MSE: 0.025353 | Test MSE: 0.055079\n",
      "Epoch 150 | Train MSE: 0.038829 | Val MSE: 0.024426 | Test MSE: 0.044092\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(101, 151):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for (x,) in train_loader:\n",
    "        x = x.to(device)\n",
    "        x_hat = model(x)\n",
    "        loss = loss_fn(x_hat, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    test_loss = evaluate(model, test_loader)\n",
    "\n",
    "    if val_loss + test_loss < best_loss:\n",
    "        best_loss = val_loss + test_loss\n",
    "        # Saves the weights\n",
    "        torch.save(model.state_dict(), encoder_path)\n",
    "        print(\"Models saved successfully.\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f} | Test MSE: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f3cc181-c113-4a1e-be3d-00b36128954b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T09:05:07.665511Z",
     "iopub.status.busy": "2025-06-27T09:05:07.664829Z",
     "iopub.status.idle": "2025-06-27T09:11:19.813081Z",
     "shell.execute_reply": "2025-06-27T09:11:19.812426Z",
     "shell.execute_reply.started": "2025-06-27T09:05:07.665463Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151 | Train MSE: 0.035499 | Val MSE: 0.026993 | Test MSE: 0.047837\n",
      "Epoch 152 | Train MSE: 0.036469 | Val MSE: 0.024778 | Test MSE: 0.043585\n",
      "Epoch 153 | Train MSE: 0.037096 | Val MSE: 0.026355 | Test MSE: 0.048484\n",
      "Epoch 154 | Train MSE: 0.033922 | Val MSE: 0.025274 | Test MSE: 0.045618\n",
      "Epoch 155 | Train MSE: 0.031814 | Val MSE: 0.025095 | Test MSE: 0.050864\n",
      "Epoch 156 | Train MSE: 0.035852 | Val MSE: 0.025641 | Test MSE: 0.047051\n",
      "Epoch 157 | Train MSE: 0.035775 | Val MSE: 0.024176 | Test MSE: 0.044478\n",
      "Epoch 158 | Train MSE: 0.032847 | Val MSE: 0.024202 | Test MSE: 0.046343\n",
      "Models saved successfully.\n",
      "Epoch 159 | Train MSE: 0.037993 | Val MSE: 0.023369 | Test MSE: 0.040921\n",
      "Models saved successfully.\n",
      "Epoch 160 | Train MSE: 0.031047 | Val MSE: 0.023494 | Test MSE: 0.039360\n",
      "Epoch 161 | Train MSE: 0.034264 | Val MSE: 0.025722 | Test MSE: 0.040771\n",
      "Epoch 162 | Train MSE: 0.034376 | Val MSE: 0.024751 | Test MSE: 0.045739\n",
      "Epoch 163 | Train MSE: 0.029455 | Val MSE: 0.023503 | Test MSE: 0.049054\n",
      "Epoch 164 | Train MSE: 0.037707 | Val MSE: 0.025693 | Test MSE: 0.045800\n",
      "Epoch 165 | Train MSE: 0.031207 | Val MSE: 0.028104 | Test MSE: 0.042886\n",
      "Epoch 166 | Train MSE: 0.033988 | Val MSE: 0.024770 | Test MSE: 0.053761\n",
      "Epoch 167 | Train MSE: 0.032545 | Val MSE: 0.024983 | Test MSE: 0.039251\n",
      "Epoch 168 | Train MSE: 0.032139 | Val MSE: 0.023923 | Test MSE: 0.041026\n",
      "Epoch 169 | Train MSE: 0.038596 | Val MSE: 0.025472 | Test MSE: 0.044804\n",
      "Epoch 170 | Train MSE: 0.032819 | Val MSE: 0.025188 | Test MSE: 0.044148\n",
      "Epoch 171 | Train MSE: 0.034638 | Val MSE: 0.026157 | Test MSE: 0.040841\n",
      "Epoch 172 | Train MSE: 0.029507 | Val MSE: 0.025773 | Test MSE: 0.048426\n",
      "Epoch 173 | Train MSE: 0.033589 | Val MSE: 0.024767 | Test MSE: 0.051077\n",
      "Models saved successfully.\n",
      "Epoch 174 | Train MSE: 0.036460 | Val MSE: 0.023187 | Test MSE: 0.036628\n",
      "Epoch 175 | Train MSE: 0.028911 | Val MSE: 0.025153 | Test MSE: 0.045083\n",
      "Epoch 176 | Train MSE: 0.030752 | Val MSE: 0.024135 | Test MSE: 0.041626\n",
      "Epoch 177 | Train MSE: 0.035213 | Val MSE: 0.024360 | Test MSE: 0.045635\n",
      "Epoch 178 | Train MSE: 0.033211 | Val MSE: 0.027603 | Test MSE: 0.045990\n",
      "Epoch 179 | Train MSE: 0.031039 | Val MSE: 0.023367 | Test MSE: 0.042083\n",
      "Epoch 180 | Train MSE: 0.033089 | Val MSE: 0.023422 | Test MSE: 0.046118\n",
      "Epoch 181 | Train MSE: 0.033170 | Val MSE: 0.022650 | Test MSE: 0.042930\n",
      "Epoch 182 | Train MSE: 0.027776 | Val MSE: 0.026189 | Test MSE: 0.045425\n",
      "Epoch 183 | Train MSE: 0.031238 | Val MSE: 0.023128 | Test MSE: 0.041965\n",
      "Epoch 184 | Train MSE: 0.033618 | Val MSE: 0.022920 | Test MSE: 0.047399\n",
      "Epoch 185 | Train MSE: 0.030887 | Val MSE: 0.023928 | Test MSE: 0.040682\n",
      "Epoch 186 | Train MSE: 0.028540 | Val MSE: 0.021241 | Test MSE: 0.039051\n",
      "Epoch 187 | Train MSE: 0.031423 | Val MSE: 0.023171 | Test MSE: 0.042322\n",
      "Epoch 188 | Train MSE: 0.028233 | Val MSE: 0.026275 | Test MSE: 0.039786\n",
      "Epoch 189 | Train MSE: 0.029114 | Val MSE: 0.025041 | Test MSE: 0.041439\n",
      "Epoch 190 | Train MSE: 0.032459 | Val MSE: 0.022582 | Test MSE: 0.041471\n",
      "Epoch 191 | Train MSE: 0.030685 | Val MSE: 0.021475 | Test MSE: 0.042783\n",
      "Epoch 192 | Train MSE: 0.031310 | Val MSE: 0.023809 | Test MSE: 0.047212\n",
      "Epoch 193 | Train MSE: 0.031545 | Val MSE: 0.022760 | Test MSE: 0.047491\n",
      "Epoch 194 | Train MSE: 0.029458 | Val MSE: 0.029130 | Test MSE: 0.046390\n",
      "Epoch 195 | Train MSE: 0.031725 | Val MSE: 0.026704 | Test MSE: 0.082530\n",
      "Epoch 196 | Train MSE: 0.033104 | Val MSE: 0.022035 | Test MSE: 0.041228\n",
      "Epoch 197 | Train MSE: 0.028156 | Val MSE: 0.022436 | Test MSE: 0.047624\n",
      "Epoch 198 | Train MSE: 0.029561 | Val MSE: 0.022122 | Test MSE: 0.043844\n",
      "Epoch 199 | Train MSE: 0.027173 | Val MSE: 0.022696 | Test MSE: 0.041963\n",
      "Epoch 200 | Train MSE: 0.029322 | Val MSE: 0.022921 | Test MSE: 0.043799\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(151, 201):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for (x,) in train_loader:\n",
    "        x = x.to(device)\n",
    "        x_hat = model(x)\n",
    "        loss = loss_fn(x_hat, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    test_loss = evaluate(model, test_loader)\n",
    "\n",
    "    if val_loss + test_loss < best_loss:\n",
    "        best_loss = val_loss + test_loss\n",
    "        # Saves the weights\n",
    "        torch.save(model.state_dict(), encoder_path)\n",
    "        print(\"Models saved successfully.\")\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f} | Test MSE: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8499e3",
   "metadata": {},
   "source": [
    "# Save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31303e8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-27T08:43:41.247265Z",
     "iopub.status.idle": "2025-06-27T08:43:41.247499Z",
     "shell.execute_reply": "2025-06-27T08:43:41.247388Z",
     "shell.execute_reply.started": "2025-06-27T08:43:41.247378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"encoder_path = \"/kaggle/working/encoder.pth\"\n",
    "\n",
    "# Saves the weights\n",
    "torch.save(model.state_dict(), encoder_path)\n",
    "\n",
    "print(\"Models saved successfully.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "446e901f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T09:11:28.279652Z",
     "iopub.status.busy": "2025-06-27T09:11:28.279343Z",
     "iopub.status.idle": "2025-06-27T09:11:28.310657Z",
     "shell.execute_reply": "2025-06-27T09:11:28.309891Z",
     "shell.execute_reply.started": "2025-06-27T09:11:28.279629Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models reloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_loaded = TCNAutoencoder(input_dim=D, emb_dim=emb_dim, seq_len=T, channels=channels)\n",
    "\n",
    "model_loaded.load_state_dict(torch.load(encoder_path))\n",
    "\n",
    "model_loaded.to(device)\n",
    "\n",
    "model_loaded.eval()\n",
    "\n",
    "print(\"Models reloaded successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7752660,
     "sourceId": 12300120,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
